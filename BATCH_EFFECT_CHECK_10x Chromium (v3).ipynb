{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"gAHHs5T1xpkh"},"source":["# Feed Forward Neural Network "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"nRJ9MX8tDYBO"},"source":["You need to have these three files to run all the cells in this notebook\n","1. ITClust train dataset\n","2. ITClust test dataset\n","3. FlowGMM codebase zip "]},{"cell_type":"code","execution_count":1,"metadata":{"id":"p-_nnfyDDDxq"},"outputs":[],"source":["all_batches_file = '/home/anunay18021/SingleCellClassification/dataset/adata_pbmc_batches_raw.h5ad'"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/anunay18021/.venv/lib/python3.8/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n","  @numba.jit()\n","/home/anunay18021/.venv/lib/python3.8/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n","  @numba.jit()\n","/home/anunay18021/.venv/lib/python3.8/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n","  @numba.jit()\n","/home/anunay18021/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.23.5 scipy==1.10.1 pandas==1.5.3 scikit-learn==1.2.2 statsmodels==0.14.0 pynndescent==0.5.10\n"]},{"name":"stderr","output_type":"stream","text":["/home/anunay18021/.venv/lib/python3.8/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n","  @numba.jit()\n"]}],"source":["# Call Libraries\n","import torch\n","from torch import nn\n","from torch.nn.parameter import Parameter\n","from torch import distributions\n","import sys\n","sys.path.insert(0, '')\n","from scripts.utils import *\n","import numpy as np\n","torch.manual_seed(0)\n","np.random.seed(0)\n","import scanpy as sc\n","sc.settings.verbosity = 3             # verbosity: errors (0), warnings (1), info (2), hints (3)\n","sc.logging.print_header()\n","import os\n","from numpy.random import seed\n","# from tensorflow import set_random_seed\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import warnings\n","import pickle\n","import sys\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from pylab import rcParams\n","rcParams['figure.dpi'] = 100\n","\n","# from flow_ssl.realnvp.realnvp_toy import ToyRealNVP\n","from flow_ssl.realnvp.realnvp import RealNVPTabular\n","from flow_ssl.data import make_circles_ssl, make_moons_ssl, make_dataset_from_img, make_dataset_from_npz\n","from flow_ssl.distributions import SSLGaussMixture\n","from flow_ssl import FlowLoss\n","\n","from itertools import chain\n","\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/anunay18021/.venv/lib/python3.8/site-packages/anndata/_core/anndata.py:117: ImplicitModificationWarning: Transforming to str index.\n","  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n","  utils.warn_names_duplicates(\"var\")\n"]}],"source":["adata = sc.read(all_batches_file)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","____Create unique index____ \n","normalizing counts per cell\n","    finished (0:00:02)\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 500x550 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["____Filtering the data____\n","pre filtering: (30495, 33694)\n","filtered out 2035 cells that have less than 200 genes expressed\n","filtered out 9096 genes that are detected in less than 3 cells\n","post filtering: (28460, 24598)\n","____Log normalizing____\n","normalizing counts per cell\n","    finished (0:00:00)\n","____Selecting highly variable genes____\n","pre: (28460, 24598)\n","If you pass `n_top_genes`, all cutoffs are ignored.\n","extracting highly variable genes\n","    finished (0:00:03)\n","--> added\n","    'highly_variable', boolean vector (adata.var)\n","    'means', float vector (adata.var)\n","    'dispersions', float vector (adata.var)\n","    'dispersions_norm', float vector (adata.var)\n","pre: (28460, 24598)\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 1280x480 with 2 Axes>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["post: (28460, 3000)\n","____Scaling the data____\n"]},{"name":"stderr","output_type":"stream","text":["/home/anunay18021/.venv/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py:843: UserWarning: Received a view of an AnnData. Making a copy.\n","  view_to_actual(adata)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th>gene_symbols</th>\n","      <th>SCYL3</th>\n","      <th>FUCA2</th>\n","      <th>TMEM176A</th>\n","      <th>HSPB6</th>\n","      <th>PDK4</th>\n","      <th>SLC22A16</th>\n","      <th>ARX</th>\n","      <th>SLC25A13</th>\n","      <th>SLC4A1</th>\n","      <th>THSD7A</th>\n","      <th>...</th>\n","      <th>CH17-262H11.1</th>\n","      <th>TRBJ1-5</th>\n","      <th>RP11-328P23.4</th>\n","      <th>CH17-212P11.4</th>\n","      <th>CH17-224D4.1</th>\n","      <th>RP11-596C23.6</th>\n","      <th>CTC-490G23.6</th>\n","      <th>PRNCR1</th>\n","      <th>RP1-273N12.4</th>\n","      <th>TRBV6-2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>pbmc1_Celseq2_1_ACAGAC</th>\n","      <td>-0.210501</td>\n","      <td>2.240501</td>\n","      <td>-0.133715</td>\n","      <td>-0.017045</td>\n","      <td>-0.092075</td>\n","      <td>-0.022733</td>\n","      <td>-0.017383</td>\n","      <td>-0.148193</td>\n","      <td>2.772827</td>\n","      <td>-0.018138</td>\n","      <td>...</td>\n","      <td>-0.016476</td>\n","      <td>-0.037941</td>\n","      <td>-0.019149</td>\n","      <td>-0.009174</td>\n","      <td>-0.018344</td>\n","      <td>-0.035799</td>\n","      <td>-0.009078</td>\n","      <td>-0.125148</td>\n","      <td>-0.020136</td>\n","      <td>-0.046757</td>\n","    </tr>\n","    <tr>\n","      <th>pbmc1_Celseq2_1_ACAGGA</th>\n","      <td>1.656750</td>\n","      <td>2.347928</td>\n","      <td>-0.133715</td>\n","      <td>-0.017045</td>\n","      <td>-0.092075</td>\n","      <td>-0.022733</td>\n","      <td>-0.017383</td>\n","      <td>-0.148193</td>\n","      <td>-0.130104</td>\n","      <td>-0.018138</td>\n","      <td>...</td>\n","      <td>-0.016476</td>\n","      <td>-0.037941</td>\n","      <td>-0.019149</td>\n","      <td>-0.009174</td>\n","      <td>-0.018344</td>\n","      <td>-0.035799</td>\n","      <td>-0.009078</td>\n","      <td>-0.125148</td>\n","      <td>-0.020136</td>\n","      <td>-0.046757</td>\n","    </tr>\n","    <tr>\n","      <th>pbmc1_Celseq2_1_ACGTTG</th>\n","      <td>-0.210501</td>\n","      <td>-0.162299</td>\n","      <td>-0.133715</td>\n","      <td>-0.017045</td>\n","      <td>-0.092075</td>\n","      <td>-0.022733</td>\n","      <td>-0.017383</td>\n","      <td>-0.148193</td>\n","      <td>-0.130104</td>\n","      <td>-0.018138</td>\n","      <td>...</td>\n","      <td>-0.016476</td>\n","      <td>-0.037941</td>\n","      <td>-0.019149</td>\n","      <td>-0.009174</td>\n","      <td>-0.018344</td>\n","      <td>-0.035799</td>\n","      <td>-0.009078</td>\n","      <td>-0.125148</td>\n","      <td>-0.020136</td>\n","      <td>-0.046757</td>\n","    </tr>\n","    <tr>\n","      <th>pbmc1_Celseq2_1_AGACCA</th>\n","      <td>-0.210501</td>\n","      <td>-0.162299</td>\n","      <td>-0.133715</td>\n","      <td>-0.017045</td>\n","      <td>-0.092075</td>\n","      <td>-0.022733</td>\n","      <td>-0.017383</td>\n","      <td>-0.148193</td>\n","      <td>-0.130104</td>\n","      <td>-0.018138</td>\n","      <td>...</td>\n","      <td>-0.016476</td>\n","      <td>-0.037941</td>\n","      <td>-0.019149</td>\n","      <td>-0.009174</td>\n","      <td>-0.018344</td>\n","      <td>-0.035799</td>\n","      <td>-0.009078</td>\n","      <td>-0.125148</td>\n","      <td>-0.020136</td>\n","      <td>-0.046757</td>\n","    </tr>\n","    <tr>\n","      <th>pbmc1_Celseq2_1_CAACTC</th>\n","      <td>-0.210501</td>\n","      <td>-0.162299</td>\n","      <td>-0.133715</td>\n","      <td>-0.017045</td>\n","      <td>-0.092075</td>\n","      <td>-0.022733</td>\n","      <td>-0.017383</td>\n","      <td>-0.148193</td>\n","      <td>-0.130104</td>\n","      <td>-0.018138</td>\n","      <td>...</td>\n","      <td>-0.016476</td>\n","      <td>-0.037941</td>\n","      <td>-0.019149</td>\n","      <td>-0.009174</td>\n","      <td>-0.018344</td>\n","      <td>-0.035799</td>\n","      <td>-0.009078</td>\n","      <td>-0.125148</td>\n","      <td>-0.020136</td>\n","      <td>-0.046757</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows  3000 columns</p>\n","</div>"],"text/plain":["gene_symbols               SCYL3     FUCA2  TMEM176A     HSPB6      PDK4  \\\n","pbmc1_Celseq2_1_ACAGAC -0.210501  2.240501 -0.133715 -0.017045 -0.092075   \n","pbmc1_Celseq2_1_ACAGGA  1.656750  2.347928 -0.133715 -0.017045 -0.092075   \n","pbmc1_Celseq2_1_ACGTTG -0.210501 -0.162299 -0.133715 -0.017045 -0.092075   \n","pbmc1_Celseq2_1_AGACCA -0.210501 -0.162299 -0.133715 -0.017045 -0.092075   \n","pbmc1_Celseq2_1_CAACTC -0.210501 -0.162299 -0.133715 -0.017045 -0.092075   \n","\n","gene_symbols            SLC22A16       ARX  SLC25A13    SLC4A1    THSD7A  ...  \\\n","pbmc1_Celseq2_1_ACAGAC -0.022733 -0.017383 -0.148193  2.772827 -0.018138  ...   \n","pbmc1_Celseq2_1_ACAGGA -0.022733 -0.017383 -0.148193 -0.130104 -0.018138  ...   \n","pbmc1_Celseq2_1_ACGTTG -0.022733 -0.017383 -0.148193 -0.130104 -0.018138  ...   \n","pbmc1_Celseq2_1_AGACCA -0.022733 -0.017383 -0.148193 -0.130104 -0.018138  ...   \n","pbmc1_Celseq2_1_CAACTC -0.022733 -0.017383 -0.148193 -0.130104 -0.018138  ...   \n","\n","gene_symbols            CH17-262H11.1   TRBJ1-5  RP11-328P23.4  CH17-212P11.4  \\\n","pbmc1_Celseq2_1_ACAGAC      -0.016476 -0.037941      -0.019149      -0.009174   \n","pbmc1_Celseq2_1_ACAGGA      -0.016476 -0.037941      -0.019149      -0.009174   \n","pbmc1_Celseq2_1_ACGTTG      -0.016476 -0.037941      -0.019149      -0.009174   \n","pbmc1_Celseq2_1_AGACCA      -0.016476 -0.037941      -0.019149      -0.009174   \n","pbmc1_Celseq2_1_CAACTC      -0.016476 -0.037941      -0.019149      -0.009174   \n","\n","gene_symbols            CH17-224D4.1  RP11-596C23.6  CTC-490G23.6    PRNCR1  \\\n","pbmc1_Celseq2_1_ACAGAC     -0.018344      -0.035799     -0.009078 -0.125148   \n","pbmc1_Celseq2_1_ACAGGA     -0.018344      -0.035799     -0.009078 -0.125148   \n","pbmc1_Celseq2_1_ACGTTG     -0.018344      -0.035799     -0.009078 -0.125148   \n","pbmc1_Celseq2_1_AGACCA     -0.018344      -0.035799     -0.009078 -0.125148   \n","pbmc1_Celseq2_1_CAACTC     -0.018344      -0.035799     -0.009078 -0.125148   \n","\n","gene_symbols            RP1-273N12.4   TRBV6-2  \n","pbmc1_Celseq2_1_ACAGAC     -0.020136 -0.046757  \n","pbmc1_Celseq2_1_ACAGGA     -0.020136 -0.046757  \n","pbmc1_Celseq2_1_ACGTTG     -0.020136 -0.046757  \n","pbmc1_Celseq2_1_AGACCA     -0.020136 -0.046757  \n","pbmc1_Celseq2_1_CAACTC     -0.020136 -0.046757  \n","\n","[5 rows x 3000 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["\n","print(\"\\n____Create unique index____ \")\n","adata.var_names_make_unique()\n","sc.pl.highest_expr_genes(adata, n_top=20, )\n","        \n","print(\"____Filtering the data____\")\n","print(\"pre filtering:\",adata.shape)\n","sc.pp.filter_cells(adata, min_genes=200)\n","sc.pp.filter_genes(adata, min_cells=3)\n","print(\"post filtering:\",adata.shape)\n","\n","print(\"____Log normalizing____\")\n","sc.pp.normalize_total(adata, target_sum=1e4)\n","sc.pp.log1p(adata)\n","\n","print(\"____Selecting highly variable genes____\")\n","print(\"pre:\",adata.shape)\n","sc.pp.highly_variable_genes(adata, min_mean=0.001 , max_mean=3, min_disp=0.3, n_top_genes=3000)\n","print(\"pre:\",adata.shape)\n","adata.raw = adata\n","adata = adata[:, adata.var.highly_variable]\n","\n","sc.pl.highly_variable_genes(adata)\n","print(\"post:\",adata.shape)\n","\n","print(\"____Scaling the data____\")\n","sc.pp.scale(adata, max_value=10)\n","adata.to_df().head()"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["all_batches = list(set(adata.obs.Method.values))"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["batch_name = '10x Chromium (v3)'"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["mapping = {}\n","reverse_mapping = {}\n","cnt = 0\n","for i in set(adata.obs.CellType.values):\n","    mapping[i] = cnt\n","    reverse_mapping[cnt] = i\n","    cnt += 1"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["total_classes = len(set(adata.obs.CellType.values))"]},{"cell_type":"markdown","metadata":{},"source":["# One vs all for batch '10x Chromium (v3)'"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["train_df = adata[adata.obs['Method'] != batch_name].to_df()\n","test_df = adata[adata.obs['Method'] == batch_name].to_df()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Taking common genes...\n","Common columns 3000\n"]}],"source":["print(\"Taking common genes...\")\n","final_columns = list(set(train_df.columns).intersection(set(test_df.columns)))\n","print('Common columns', len(final_columns))\n","final_columns = [i for i in final_columns if i != 'CellType'] \n","train_df = train_df[final_columns]\n","test_df = test_df[final_columns]\n","\n","y_train = adata[adata.obs['Method'] != batch_name].obs.CellType.to_list()\n","y_test = adata[adata.obs['Method'] == batch_name].obs.CellType.to_list()\n","\n","X_train = train_df.to_numpy()\n","X_test = test_df.to_numpy()"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["y_test_lab = convert_y_to_mapping(y_test, mapping)\n","y_test_lab = np.array(y_test_lab)\n","\n","y_train_lab = convert_y_to_mapping(y_train, mapping)\n","y_train_lab = np.array(y_train_lab)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["with open('/home/anunay18021/SingleCellClassification/dataset/np/X_train_'+batch_name+'.pkl', 'wb') as fh:\n","        pickle.dump(X_train, fh)\n","\n","with open('/home/anunay18021/SingleCellClassification/dataset/np/X_test_'+batch_name+'.pkl', 'wb') as fh:\n","    pickle.dump(X_test, fh)\n","\n","with open('/home/anunay18021/SingleCellClassification/dataset/np/y_test_'+batch_name+'.pkl', 'wb') as fh:\n","    pickle.dump(y_test_lab, fh)\n","\n","with open('/home/anunay18021/SingleCellClassification/dataset/np/y_train_'+batch_name+'.pkl', 'wb') as fh:\n","    pickle.dump(y_train_lab, fh)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["25466"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["len(y_train_lab)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/anunay18021/SingleCellClassification/flowgmm\n"]}],"source":["%cd flowgmm"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n0Zfu1blzejB","outputId":"ecf9742d-4067-4ad6-e529-2bcf3bf0669b"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'num_classes': 10, 'metric_name': '10x Chromium (v3)', 'dataset': <class 'flow_ssl.data.nlp_datasets.AG_News'>, 'network': <function RealNVPTabularWPrior at 0x7fc339516dc0>, 'num_epochs': 500, 'bs': 5000, 'lr': 0.0003, 'optim': <class 'torch.optim.adamw.AdamW'>, 'device': 'cuda', 'trainer': SemiFlow, 'split': {'train': 200, 'val': 5000}, 'net_config': {'k': 1024, 'coupling_layers': 7, 'nperlayer': 1}, 'opt_config': {'weight_decay': 1e-05}, 'trainer_config': {'log_dir': '/home/anunay18021/tb-experiments/UCI/', 'log_args': {'minPeriod': 0.1, 'timeFrac': 0.3}, 'unlab_weight': 0.6}, 'save': False}\n","10x Chromium (v3)\n","25466\n","25466\n","2994\n","Pairwise dists: [[ 0.         42.5923555  43.29036067 42.50096539 42.7565425  44.12983675\n","  43.23289163 43.06433303 43.32655576 43.66125978]\n"," [42.5923555   0.         43.39948197 42.52300032 43.19421944 43.11323417\n","  42.85981378 42.56115329 42.95834696 43.1520443 ]\n"," [43.29036067 43.39948197  0.         42.98435597 44.29631384 44.07753256\n","  43.10195656 43.22908566 43.594617   44.00092128]\n"," [42.50096539 42.52300032 42.98435597  0.         43.18481895 43.66142401\n","  42.76014959 42.57990605 43.35093558 43.32573681]\n"," [42.7565425  43.19421944 44.29631384 43.18481895  0.         43.92187234\n","  43.05041261 43.87749509 43.01877217 43.06207   ]\n"," [44.12983675 43.11323417 44.07753256 43.66142401 43.92187234  0.\n","  43.98973875 44.50644793 43.06888124 43.67429094]\n"," [43.23289163 42.85981378 43.10195656 42.76014959 43.05041261 43.98973875\n","   0.         42.63241925 43.30691781 43.07846737]\n"," [43.06433303 42.56115329 43.22908566 42.57990605 43.87749509 44.50644793\n","  42.63241925  0.         42.8821537  43.40904866]\n"," [43.32655576 42.95834696 43.594617   43.35093558 43.01877217 43.06888124\n","  43.30691781 42.8821537   0.         43.17272514]\n"," [43.66125978 43.1520443  44.00092128 43.32573681 43.06207    43.67429094\n","  43.07846737 43.40904866 43.17272514  0.        ]]\n","10 10x Chromium (v3)\n","train:   0%|                                            | 0/500 [00:00<?, ?it/s]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 10.023601458928574, 'Train_Acc': 0.08595805412812607, 'val_Acc': 0.08166470357283079, 'test_Acc': 0.08383433533734135, 'class_Acc_0': nan, 'class_Acc_1': 0.1288659793814433, 'class_Acc_2': 0.03333333333333334, 'class_Acc_3': 0.030612244897959183, 'class_Acc_4': 0.08004158004158003, 'class_Acc_5': 0.04761904761904762, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.2344632768361582, 'class_Acc_8': 0.08092485549132948, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(4096.2617, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","   Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc    val_bpd\n","0     6701.502441   0.085958     4096.261719  ...  0.083834  0.081665  10.023601\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:   1%|                                   | 3/500 [00:21<54:22,  6.56s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 8.710854931377366, 'Train_Acc': 0.35890282973621107, 'val_Acc': 0.35335689045936397, 'test_Acc': 0.42651970607882433, 'class_Acc_0': nan, 'class_Acc_1': 0.0, 'class_Acc_2': 0.0010416666666666669, 'class_Acc_3': 0.0, 'class_Acc_4': 1.0, 'class_Acc_5': 0.0, 'class_Acc_6': 0.0, 'class_Acc_7': 0.8870056497175142, 'class_Acc_8': 0.0, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(1400.6255, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","    Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","23     2452.815918   0.358903     1400.625488  ...   0.42652  0.353357  8.710855\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:   1%|                                   | 7/500 [00:50<52:07,  6.34s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 8.35461078382797, 'Train_Acc': 0.5192137718396711, 'val_Acc': 0.5056929721240675, 'test_Acc': 0.5076820307281229, 'class_Acc_0': nan, 'class_Acc_1': 0.0, 'class_Acc_2': 0.0031249999999999997, 'class_Acc_3': 0.0, 'class_Acc_4': 0.9958419958419957, 'class_Acc_5': 0.0, 'class_Acc_6': 0.0, 'class_Acc_7': 1.0, 'class_Acc_8': 0.592485549132948, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(603.5967, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","    Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","46     1331.300537   0.519214       603.59668  ...  0.507682  0.505693  8.354611\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:   2%|                                  | 11/500 [01:16<49:26,  6.07s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 8.084716690919697, 'Train_Acc': 0.6267513394998288, 'val_Acc': 0.6215155084413035, 'test_Acc': 0.5541082164328658, 'class_Acc_0': nan, 'class_Acc_1': 0.0, 'class_Acc_2': 0.0041666666666666675, 'class_Acc_3': 0.0, 'class_Acc_4': 0.9979209979209978, 'class_Acc_5': 0.0, 'class_Acc_6': 0.0, 'class_Acc_7': 1.0, 'class_Acc_8': 0.985549132947977, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(5.513775, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","    Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","69      524.782898   0.626751        5.513775  ...  0.554108  0.621516  8.084717\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:   3%|                                  | 15/500 [01:43<48:35,  6.01s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 7.87057667673318, 'Train_Acc': 0.6476259677971908, 'val_Acc': 0.6395759717314488, 'test_Acc': 0.5564462257849031, 'class_Acc_0': nan, 'class_Acc_1': 0.0, 'class_Acc_2': 0.008333333333333335, 'class_Acc_3': 0.0, 'class_Acc_4': 0.9989604989604989, 'class_Acc_5': 0.0, 'class_Acc_6': 0.0, 'class_Acc_7': 1.0, 'class_Acc_8': 0.9913294797687862, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(66.11112, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","    Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","92      109.575897   0.647626       66.111122  ...  0.556446  0.639576  7.870577\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:   4%|                                 | 19/500 [02:10<48:12,  6.01s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 7.713439883823687, 'Train_Acc': 0.7199719767043509, 'val_Acc': 0.7149587750294464, 'test_Acc': 0.6042084168336673, 'class_Acc_0': nan, 'class_Acc_1': 0.0, 'class_Acc_2': 0.16145833333333334, 'class_Acc_3': 0.0, 'class_Acc_4': 0.9958419958419957, 'class_Acc_5': 0.0, 'class_Acc_6': 0.0, 'class_Acc_7': 1.0, 'class_Acc_8': 0.9884393063583817, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-969.7424, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc  val_bpd\n","115     -844.533447   0.719972     -969.742371  ...  0.604208  0.714959  7.71344\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:   4%|                                 | 22/500 [02:34<56:41,  7.12s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 7.498893884181201, 'Train_Acc': 0.7573235354573483, 'val_Acc': 0.740478994895956, 'test_Acc': 0.6516366065464262, 'class_Acc_0': nan, 'class_Acc_1': 0.0, 'class_Acc_2': 0.29270833333333335, 'class_Acc_3': 0.0, 'class_Acc_4': 0.9958419958419957, 'class_Acc_5': 0.380952380952381, 'class_Acc_6': 0.0, 'class_Acc_7': 1.0, 'class_Acc_8': 0.9884393063583817, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-768.59265, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","134    -1009.322021   0.757324     -768.592651  ...  0.651637  0.740479  7.498894\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:   5%|                               | 24/500 [03:01<1:19:37, 10.04s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 7.386517239116331, 'Train_Acc': 0.805527344981158, 'val_Acc': 0.7762073027090695, 'test_Acc': 0.7578490313961256, 'class_Acc_0': nan, 'class_Acc_1': 0.0, 'class_Acc_2': 0.6260416666666667, 'class_Acc_3': 0.0, 'class_Acc_4': 0.974012474012474, 'class_Acc_5': 0.9047619047619047, 'class_Acc_6': 0.0, 'class_Acc_7': 1.0, 'class_Acc_8': 0.9797687861271677, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-2287.913, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","149    -2090.200195   0.805527    -2287.913086  ...  0.757849  0.776207  7.386517\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:   5%|                               | 27/500 [03:35<1:21:38, 10.36s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 7.263826228541643, 'Train_Acc': 0.8059451455978074, 'val_Acc': 0.7852375343541421, 'test_Acc': 0.7588510354041417, 'class_Acc_0': nan, 'class_Acc_1': 0.0, 'class_Acc_2': 0.6208333333333333, 'class_Acc_3': 0.0, 'class_Acc_4': 0.9760914760914758, 'class_Acc_5': 0.9761904761904762, 'class_Acc_6': 0.0, 'class_Acc_7': 0.9971751412429379, 'class_Acc_8': 0.9913294797687862, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-1732.7563, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","165    -2016.087891   0.805945    -1732.756348  ...  0.758851  0.785238  7.263826\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:   6%|                               | 30/500 [04:11<1:23:13, 10.62s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 7.205965680629376, 'Train_Acc': 0.8156157588215142, 'val_Acc': 0.7875932469572046, 'test_Acc': 0.7785571142284569, 'class_Acc_0': nan, 'class_Acc_1': 0.0, 'class_Acc_2': 0.6958333333333334, 'class_Acc_3': 0.0, 'class_Acc_4': 0.9646569646569645, 'class_Acc_5': 0.9761904761904762, 'class_Acc_6': 0.0, 'class_Acc_7': 0.9971751412429379, 'class_Acc_8': 0.985549132947977, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-2081.8406, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","181    -2411.010498   0.815616    -2081.840576  ...  0.778557  0.787593  7.205966\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:   7%|                              | 33/500 [04:46<1:23:06, 10.68s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 7.118543914494231, 'Train_Acc': 0.807420363138061, 'val_Acc': 0.7781703965449549, 'test_Acc': 0.7668670674682698, 'class_Acc_0': nan, 'class_Acc_1': 0.0, 'class_Acc_2': 0.6708333333333334, 'class_Acc_3': 0.010204081632653062, 'class_Acc_4': 0.9490644490644489, 'class_Acc_5': 1.0, 'class_Acc_6': 0.0, 'class_Acc_7': 0.9971751412429379, 'class_Acc_8': 0.9913294797687862, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-2391.478, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc  val_Acc   val_bpd\n","198    -2769.281982    0.80742    -2391.478027  ...  0.766867  0.77817  7.118544\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:   7%|                              | 35/500 [05:23<1:50:15, 14.23s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 7.0338648304475235, 'Train_Acc': 0.8204632271325797, 'val_Acc': 0.7926972909305064, 'test_Acc': 0.7935871743486974, 'class_Acc_0': nan, 'class_Acc_1': 0.0, 'class_Acc_2': 0.7979166666666667, 'class_Acc_3': 0.0, 'class_Acc_4': 0.9095634095634095, 'class_Acc_5': 1.0, 'class_Acc_6': 0.0, 'class_Acc_7': 0.9971751412429379, 'class_Acc_8': 0.9826589595375722, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-2572.848, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","210    -3049.485352   0.820463      -2572.8479  ...  0.793587  0.792697  7.033865\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:   7%|                              | 37/500 [06:04<2:07:58, 16.58s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 6.971795424041335, 'Train_Acc': 0.8237792120589243, 'val_Acc': 0.7919120533961523, 'test_Acc': 0.7882431529726119, 'class_Acc_0': nan, 'class_Acc_1': 0.0, 'class_Acc_2': 0.7354166666666667, 'class_Acc_3': 0.010204081632653062, 'class_Acc_4': 0.9521829521829521, 'class_Acc_5': 1.0, 'class_Acc_6': 0.0, 'class_Acc_7': 0.9971751412429379, 'class_Acc_8': 0.9884393063583817, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-1938.5211, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","224    -2730.627441   0.823779    -1938.521118  ...  0.788243  0.791912  6.971795\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:   8%|                              | 39/500 [06:52<2:31:58, 19.78s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 6.906353330534551, 'Train_Acc': 0.8251468585131895, 'val_Acc': 0.7942677659992148, 'test_Acc': 0.7915831663326653, 'class_Acc_0': nan, 'class_Acc_1': 0.0, 'class_Acc_2': 0.8156249999999999, 'class_Acc_3': 0.010204081632653062, 'class_Acc_4': 0.8835758835758837, 'class_Acc_5': 1.0, 'class_Acc_6': 0.0, 'class_Acc_7': 0.9943502824858758, 'class_Acc_8': 0.9884393063583817, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-2712.7449, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","235    -3307.358398   0.825147    -2712.744873  ...  0.791583  0.794268  6.906353\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:   8%|                              | 41/500 [07:44<2:49:07, 22.11s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 6.86036214279918, 'Train_Acc': 0.8282280095923261, 'val_Acc': 0.7934825284648607, 'test_Acc': 0.7879091516366066, 'class_Acc_0': nan, 'class_Acc_1': 0.0, 'class_Acc_2': 0.7864583333333334, 'class_Acc_3': 0.0, 'class_Acc_4': 0.9064449064449064, 'class_Acc_5': 1.0, 'class_Acc_6': 0.0, 'class_Acc_7': 0.9915254237288137, 'class_Acc_8': 0.9797687861271677, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-2805.029, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","247    -3448.981445   0.828228    -2805.029053  ...  0.787909  0.793483  6.860362\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:   9%|                              | 43/500 [08:36<2:56:52, 23.22s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 6.838409884663989, 'Train_Acc': 0.8308838917437478, 'val_Acc': 0.7919120533961523, 'test_Acc': 0.7862391449565799, 'class_Acc_0': nan, 'class_Acc_1': 0.0, 'class_Acc_2': 0.7614583333333333, 'class_Acc_3': 0.030612244897959183, 'class_Acc_4': 0.9209979209979209, 'class_Acc_5': 1.0, 'class_Acc_6': 0.0, 'class_Acc_7': 0.9915254237288137, 'class_Acc_8': 0.985549132947977, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-2815.239, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc  val_bpd\n","259    -3612.911133   0.830884    -2815.239014  ...  0.786239  0.791912  6.83841\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:   9%|                              | 45/500 [09:28<3:01:10, 23.89s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 6.78979923330091, 'Train_Acc': 0.8254406440561837, 'val_Acc': 0.7907341970946211, 'test_Acc': 0.7915831663326653, 'class_Acc_0': nan, 'class_Acc_1': 0.0, 'class_Acc_2': 0.8541666666666666, 'class_Acc_3': 0.030612244897959183, 'class_Acc_4': 0.8503118503118502, 'class_Acc_5': 1.0, 'class_Acc_6': 0.0, 'class_Acc_7': 0.9887005649717514, 'class_Acc_8': 0.9739884393063585, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-2360.6755, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","272     -3411.51416   0.825441    -2360.675537  ...  0.791583  0.790734  6.789799\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:   9%|                              | 47/500 [10:19<3:02:22, 24.16s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 6.708233425979401, 'Train_Acc': 0.8277250428228846, 'val_Acc': 0.7868080094228505, 'test_Acc': 0.7732130928523714, 'class_Acc_0': nan, 'class_Acc_1': 0.0, 'class_Acc_2': 0.7010416666666666, 'class_Acc_3': 0.030612244897959183, 'class_Acc_4': 0.9438669438669439, 'class_Acc_5': 1.0, 'class_Acc_6': 0.0, 'class_Acc_7': 0.9858757062146893, 'class_Acc_8': 0.9826589595375722, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-2906.0356, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","285    -3842.346924   0.827725    -2906.035645  ...  0.773213  0.786808  6.708233\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  10%|                             | 49/500 [11:12<3:03:50, 24.46s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 6.6690746109010535, 'Train_Acc': 0.8351009249743062, 'val_Acc': 0.7978013349038084, 'test_Acc': 0.7842351369405478, 'class_Acc_0': nan, 'class_Acc_1': 0.005154639175257732, 'class_Acc_2': 0.8166666666666667, 'class_Acc_3': 0.020408163265306124, 'class_Acc_4': 0.87006237006237, 'class_Acc_5': 0.9523809523809524, 'class_Acc_6': 0.0, 'class_Acc_7': 0.9774011299435027, 'class_Acc_8': 0.9768786127167631, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-3642.4436, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","299    -4431.386719   0.835101    -3642.443604  ...  0.784235  0.797801  6.669075\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  10%|                             | 52/500 [12:20<2:44:45, 22.06s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 6.645431577107995, 'Train_Acc': 0.8381501062007537, 'val_Acc': 0.7993718099725167, 'test_Acc': 0.7872411489645958, 'class_Acc_0': nan, 'class_Acc_1': 0.0, 'class_Acc_2': 0.7895833333333334, 'class_Acc_3': 0.020408163265306124, 'class_Acc_4': 0.9158004158004156, 'class_Acc_5': 0.9523809523809524, 'class_Acc_6': 0.0, 'class_Acc_7': 0.980225988700565, 'class_Acc_8': 0.9508670520231214, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-3221.8323, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","313    -4258.598633    0.83815    -3221.832275  ...  0.787241  0.799372  6.645432\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  11%|                             | 54/500 [13:12<2:51:43, 23.10s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 6.634619817592584, 'Train_Acc': 0.824462076053443, 'val_Acc': 0.7852375343541421, 'test_Acc': 0.7775551102204409, 'class_Acc_0': nan, 'class_Acc_1': 0.020618556701030927, 'class_Acc_2': 0.884375, 'class_Acc_3': 0.030612244897959183, 'class_Acc_4': 0.792099792099792, 'class_Acc_5': 0.9523809523809524, 'class_Acc_6': 0.0, 'class_Acc_7': 0.9745762711864406, 'class_Acc_8': 0.9393063583815029, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-3030.8875, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc  val_bpd\n","327    -4199.993164   0.824462    -3030.887451  ...  0.777555  0.785238  6.63462\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  11%|                             | 57/500 [14:21<2:41:03, 21.81s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 6.547622770639602, 'Train_Acc': 0.8421330729701952, 'val_Acc': 0.8029053788771103, 'test_Acc': 0.782565130260521, 'class_Acc_0': nan, 'class_Acc_1': 0.0, 'class_Acc_2': 0.8177083333333335, 'class_Acc_3': 0.04081632653061225, 'class_Acc_4': 0.8690228690228688, 'class_Acc_5': 0.9523809523809524, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.9717514124293786, 'class_Acc_8': 0.9624277456647399, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-3561.2695, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","342    -4673.873047   0.842133    -3561.269531  ...  0.782565  0.802905  6.547623\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  12%|                             | 59/500 [15:12<2:49:23, 23.05s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 6.514827640180902, 'Train_Acc': 0.8418538403562864, 'val_Acc': 0.7974087161366313, 'test_Acc': 0.7835671342685371, 'class_Acc_0': nan, 'class_Acc_1': 0.005154639175257732, 'class_Acc_2': 0.8072916666666666, 'class_Acc_3': 0.030612244897959183, 'class_Acc_4': 0.8814968814968814, 'class_Acc_5': 0.9761904761904762, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.9717514124293786, 'class_Acc_8': 0.9595375722543352, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-3301.4775, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","357    -4520.820312   0.841854    -3301.477539  ...  0.783567  0.797409  6.514828\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  12%|                             | 62/500 [16:23<2:39:51, 21.90s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 6.496320333045854, 'Train_Acc': 0.84348197327852, 'val_Acc': 0.7989791912053397, 'test_Acc': 0.7852371409485638, 'class_Acc_0': nan, 'class_Acc_1': 0.0, 'class_Acc_2': 0.8166666666666667, 'class_Acc_3': 0.030612244897959183, 'class_Acc_4': 0.8804573804573805, 'class_Acc_5': 0.9523809523809524, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.9632768361581922, 'class_Acc_8': 0.9653179190751446, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-3544.2476, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc  val_bpd\n","373    -4737.373047   0.843482    -3544.247559  ...  0.785237  0.798979  6.49632\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  13%|                            | 64/500 [17:14<2:47:03, 22.99s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 6.43104707055053, 'Train_Acc': 0.8442982391229873, 'val_Acc': 0.8017275225755791, 'test_Acc': 0.7752171008684035, 'class_Acc_0': nan, 'class_Acc_1': 0.0, 'class_Acc_2': 0.8375, 'class_Acc_3': 0.030612244897959183, 'class_Acc_4': 0.8378378378378377, 'class_Acc_5': 0.9285714285714285, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.9632768361581922, 'class_Acc_8': 0.9450867052023122, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-4108.6694, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","389    -5193.213867   0.844298    -4108.669434  ...  0.775217  0.801728  6.431047\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  13%|                            | 67/500 [18:22<2:35:16, 21.52s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 6.416962948064769, 'Train_Acc': 0.846599287427201, 'val_Acc': 0.7978013349038084, 'test_Acc': 0.7745490981963928, 'class_Acc_0': nan, 'class_Acc_1': 0.010309278350515464, 'class_Acc_2': 0.7625000000000001, 'class_Acc_3': 0.05102040816326531, 'class_Acc_4': 0.8970893970893968, 'class_Acc_5': 0.9523809523809524, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.9661016949152543, 'class_Acc_8': 0.9624277456647399, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-3514.2576, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","405    -4870.670898   0.846599    -3514.257568  ...  0.774549  0.797801  6.416963\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  14%|                            | 70/500 [19:31<2:31:30, 21.14s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 6.374517563790489, 'Train_Acc': 0.8505422541966426, 'val_Acc': 0.8036906164114644, 'test_Acc': 0.7835671342685371, 'class_Acc_0': nan, 'class_Acc_1': 0.02577319587628866, 'class_Acc_2': 0.8125, 'class_Acc_3': 0.04081632653061225, 'class_Acc_4': 0.8783783783783783, 'class_Acc_5': 0.9523809523809524, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.9604519774011299, 'class_Acc_8': 0.9566473988439306, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-3788.2432, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","421    -5186.759277   0.850542    -3788.243164  ...  0.783567  0.803691  6.374518\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  14%|                            | 72/500 [20:23<2:42:55, 22.84s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 6.3291637776971275, 'Train_Acc': 0.8491445563549161, 'val_Acc': 0.8036906164114644, 'test_Acc': 0.7788911155644622, 'class_Acc_0': nan, 'class_Acc_1': 0.005154639175257732, 'class_Acc_2': 0.7885416666666667, 'class_Acc_3': 0.05102040816326531, 'class_Acc_4': 0.8908523908523908, 'class_Acc_5': 0.9523809523809524, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.9604519774011299, 'class_Acc_8': 0.9566473988439306, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-4329.23, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","437    -5622.106934   0.849145     -4329.22998  ...  0.778891  0.803691  6.329164\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  15%|                            | 75/500 [21:32<2:34:07, 21.76s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 6.294786355485006, 'Train_Acc': 0.8510946077423776, 'val_Acc': 0.806831566548881, 'test_Acc': 0.7752171008684035, 'class_Acc_0': nan, 'class_Acc_1': 0.015463917525773191, 'class_Acc_2': 0.7760416666666666, 'class_Acc_3': 0.05102040816326531, 'class_Acc_4': 0.899168399168399, 'class_Acc_5': 0.9523809523809524, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.9548022598870058, 'class_Acc_8': 0.9364161849710981, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-3772.148, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","453    -5275.336914   0.851095    -3772.147949  ...  0.775217  0.806832  6.294786\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  16%|                           | 78/500 [22:41<2:29:06, 21.20s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 6.266419575575808, 'Train_Acc': 0.8522637889688249, 'val_Acc': 0.801334903808402, 'test_Acc': 0.769873079492318, 'class_Acc_0': nan, 'class_Acc_1': 0.010309278350515464, 'class_Acc_2': 0.8354166666666667, 'class_Acc_3': 0.05102040816326531, 'class_Acc_4': 0.8222453222453221, 'class_Acc_5': 0.9285714285714285, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.9435028248587571, 'class_Acc_8': 0.9566473988439306, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-3503.6448, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc  val_bpd\n","470    -5176.967773   0.852264    -3503.644775  ...  0.769873  0.801335  6.26642\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  16%|                           | 81/500 [23:49<2:26:12, 20.94s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 6.255636111104737, 'Train_Acc': 0.8537248372730387, 'val_Acc': 0.8048684727129957, 'test_Acc': 0.7695390781563126, 'class_Acc_0': nan, 'class_Acc_1': 0.02577319587628866, 'class_Acc_2': 0.821875, 'class_Acc_3': 0.05102040816326531, 'class_Acc_4': 0.8357588357588357, 'class_Acc_5': 0.9523809523809524, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.9265536723163843, 'class_Acc_8': 0.9595375722543352, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-4041.6982, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","487    -5565.381348   0.853725    -4041.698242  ...  0.769539  0.804868  6.255636\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  17%|                           | 84/500 [24:57<2:23:38, 20.72s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 6.20638207035168, 'Train_Acc': 0.8519168071257279, 'val_Acc': 0.8001570475068708, 'test_Acc': 0.7658650634602538, 'class_Acc_0': nan, 'class_Acc_1': 0.056701030927835044, 'class_Acc_2': 0.8520833333333333, 'class_Acc_3': 0.05102040816326531, 'class_Acc_4': 0.7837837837837837, 'class_Acc_5': 0.9523809523809524, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.9435028248587571, 'class_Acc_8': 0.953757225433526, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-4265.7344, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","504    -5824.232422   0.851917    -4265.734375  ...  0.765865  0.800157  6.206382\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  17%|                           | 86/500 [25:51<2:38:41, 23.00s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 6.160988835565792, 'Train_Acc': 0.8605395683453237, 'val_Acc': 0.8060463290145269, 'test_Acc': 0.7665330661322646, 'class_Acc_0': nan, 'class_Acc_1': 0.03608247422680412, 'class_Acc_2': 0.8177083333333335, 'class_Acc_3': 0.12244897959183673, 'class_Acc_4': 0.8336798336798337, 'class_Acc_5': 0.9523809523809524, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.9180790960451978, 'class_Acc_8': 0.9335260115606937, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-4659.4346, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","521    -6162.144043    0.86054     -4659.43457  ...  0.766533  0.806046  6.160989\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  18%|                           | 89/500 [26:59<2:28:59, 21.75s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 6.164997456723106, 'Train_Acc': 0.8600674203494347, 'val_Acc': 0.8036906164114644, 'test_Acc': 0.7651970607882431, 'class_Acc_0': nan, 'class_Acc_1': 0.061855670103092765, 'class_Acc_2': 0.8114583333333335, 'class_Acc_3': 0.12244897959183673, 'class_Acc_4': 0.843035343035343, 'class_Acc_5': 0.9523809523809524, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.8954802259887007, 'class_Acc_8': 0.9219653179190752, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-4087.071, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","538    -5747.896484   0.860067    -4087.071045  ...  0.765197  0.803691  6.164997\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  18%|                           | 92/500 [28:08<2:25:31, 21.40s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 6.113562701639849, 'Train_Acc': 0.8592258855772524, 'val_Acc': 0.806831566548881, 'test_Acc': 0.7635270541082164, 'class_Acc_0': nan, 'class_Acc_1': 0.05154639175257732, 'class_Acc_2': 0.8666666666666669, 'class_Acc_3': 0.10204081632653061, 'class_Acc_4': 0.7931392931392931, 'class_Acc_5': 0.9523809523809524, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.8870056497175142, 'class_Acc_8': 0.9132947976878613, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-4163.8623, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","555    -5916.453613   0.859226    -4163.862305  ...  0.763527  0.806832  6.113563\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  19%|                          | 95/500 [29:17<2:23:00, 21.19s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 6.079617923570444, 'Train_Acc': 0.861980335731415, 'val_Acc': 0.8072241853160581, 'test_Acc': 0.7565130260521042, 'class_Acc_0': nan, 'class_Acc_1': 0.04639175257731958, 'class_Acc_2': 0.8479166666666668, 'class_Acc_3': 0.11224489795918367, 'class_Acc_4': 0.79002079002079, 'class_Acc_5': 0.8809523809523809, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.8983050847457628, 'class_Acc_8': 0.9104046242774566, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-3907.8604, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","572    -5819.183594    0.86198    -3907.860352  ...  0.756513  0.807224  6.079618\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  20%|                          | 98/500 [30:26<2:21:28, 21.12s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 6.052150597332165, 'Train_Acc': 0.8605770880438507, 'val_Acc': 0.8040832351786416, 'test_Acc': 0.7548430193720775, 'class_Acc_0': nan, 'class_Acc_1': 0.08762886597938145, 'class_Acc_2': 0.7770833333333333, 'class_Acc_3': 0.09183673469387756, 'class_Acc_4': 0.8357588357588357, 'class_Acc_5': 0.9047619047619047, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.9152542372881356, 'class_Acc_8': 0.9277456647398843, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-4484.1104, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","589    -6241.632324   0.860577    -4484.110352  ...  0.754843  0.804083  6.052151\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  20%|                         | 101/500 [31:36<2:21:10, 21.23s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 6.009638525982428, 'Train_Acc': 0.8641533538883179, 'val_Acc': 0.8087946603847664, 'test_Acc': 0.7575150300601202, 'class_Acc_0': nan, 'class_Acc_1': 0.04639175257731958, 'class_Acc_2': 0.8229166666666666, 'class_Acc_3': 0.09183673469387756, 'class_Acc_4': 0.818087318087318, 'class_Acc_5': 0.9285714285714285, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.8983050847457628, 'class_Acc_8': 0.9104046242774566, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-4677.467, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","606    -6459.890625   0.864153    -4677.466797  ...  0.757515  0.808795  6.009639\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  21%|                         | 104/500 [32:44<2:18:49, 21.03s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.984554088623534, 'Train_Acc': 0.8657066529633437, 'val_Acc': 0.8084020416175893, 'test_Acc': 0.7508350033400134, 'class_Acc_0': nan, 'class_Acc_1': 0.061855670103092765, 'class_Acc_2': 0.828125, 'class_Acc_3': 0.11224489795918367, 'class_Acc_4': 0.7941787941787941, 'class_Acc_5': 0.9047619047619047, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.8954802259887007, 'class_Acc_8': 0.8959537572254336, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-4723.4604, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","624    -6543.193848   0.865707    -4723.460449  ...  0.750835  0.808402  5.984554\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  21%|                         | 107/500 [33:53<2:17:32, 21.00s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.962973070862631, 'Train_Acc': 0.8615826378896881, 'val_Acc': 0.8036906164114644, 'test_Acc': 0.7454909819639278, 'class_Acc_0': nan, 'class_Acc_1': 0.02577319587628866, 'class_Acc_2': 0.7479166666666666, 'class_Acc_3': 0.11224489795918367, 'class_Acc_4': 0.8648648648648649, 'class_Acc_5': 0.8809523809523809, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.8870056497175142, 'class_Acc_8': 0.9046242774566474, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-4768.8984, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","642    -6626.866699   0.861583    -4768.898438  ...  0.745491  0.803691  5.962973\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  22%|                         | 109/500 [34:45<2:28:39, 22.81s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.943470623495927, 'Train_Acc': 0.8610725865022267, 'val_Acc': 0.8005496662740479, 'test_Acc': 0.7471609886439545, 'class_Acc_0': nan, 'class_Acc_1': 0.03608247422680412, 'class_Acc_2': 0.7458333333333333, 'class_Acc_3': 0.11224489795918367, 'class_Acc_4': 0.8596673596673595, 'class_Acc_5': 0.9285714285714285, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.8898305084745762, 'class_Acc_8': 0.9277456647398843, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5118.0195, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc  val_Acc   val_bpd\n","659    -6910.271484   0.861073    -5118.019531  ...  0.747161  0.80055  5.943471\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  22%|                        | 112/500 [35:54<2:21:20, 21.86s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.922398212092214, 'Train_Acc': 0.8670856046591299, 'val_Acc': 0.8076168040832352, 'test_Acc': 0.741816967267869, 'class_Acc_0': nan, 'class_Acc_1': 0.09278350515463916, 'class_Acc_2': 0.8395833333333332, 'class_Acc_3': 0.07142857142857142, 'class_Acc_4': 0.765072765072765, 'class_Acc_5': 0.8809523809523809, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.8870056497175142, 'class_Acc_8': 0.8728323699421967, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-4614.1045, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","676    -6549.188477   0.867086    -4614.104492  ...  0.741817  0.807617  5.922398\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  23%|                        | 115/500 [37:03<2:16:15, 21.24s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.911807412217668, 'Train_Acc': 0.8697529701952723, 'val_Acc': 0.81232822928936, 'test_Acc': 0.741816967267869, 'class_Acc_0': nan, 'class_Acc_1': 0.09278350515463916, 'class_Acc_2': 0.8260416666666666, 'class_Acc_3': 0.09183673469387756, 'class_Acc_4': 0.7765072765072765, 'class_Acc_5': 0.9285714285714285, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.8954802259887007, 'class_Acc_8': 0.8583815028901735, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-4632.263, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","694    -6590.150391   0.869753    -4632.263184  ...  0.741817  0.812328  5.911807\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  24%|                        | 118/500 [38:11<2:13:33, 20.98s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.86182239658468, 'Train_Acc': 0.869060335731415, 'val_Acc': 0.811935610522183, 'test_Acc': 0.7458249832999332, 'class_Acc_0': nan, 'class_Acc_1': 0.05154639175257732, 'class_Acc_2': 0.8156249999999999, 'class_Acc_3': 0.1326530612244898, 'class_Acc_4': 0.8097713097713097, 'class_Acc_5': 0.9047619047619047, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.8531073446327684, 'class_Acc_8': 0.8843930635838151, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-4739.4316, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","712    -6752.344727    0.86906    -4739.431641  ...  0.745825  0.811936  5.861822\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  24%|                        | 121/500 [39:20<2:11:57, 20.89s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.857196332945267, 'Train_Acc': 0.869513353888318, 'val_Acc': 0.8135060855908912, 'test_Acc': 0.7424849699398798, 'class_Acc_0': nan, 'class_Acc_1': 0.09793814432989689, 'class_Acc_2': 0.809375, 'class_Acc_3': 0.1326530612244898, 'class_Acc_4': 0.7962577962577961, 'class_Acc_5': 0.9047619047619047, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.8559322033898306, 'class_Acc_8': 0.8843930635838151, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-4741.211, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","730    -6807.060547   0.869513    -4741.210938  ...  0.742485  0.813506  5.857196\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  25%|                        | 124/500 [40:29<2:11:23, 20.97s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.820896726227171, 'Train_Acc': 0.8696658855772524, 'val_Acc': 0.8091872791519434, 'test_Acc': 0.7414829659318637, 'class_Acc_0': nan, 'class_Acc_1': 0.08247422680412371, 'class_Acc_2': 0.7916666666666666, 'class_Acc_3': 0.11224489795918367, 'class_Acc_4': 0.8191268191268192, 'class_Acc_5': 0.8571428571428571, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.8644067796610171, 'class_Acc_8': 0.8699421965317918, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-4820.5728, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","748    -6891.946289   0.869666    -4820.572754  ...  0.741483  0.809187  5.820897\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  25%|                       | 127/500 [41:39<2:11:26, 21.14s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.814239759363693, 'Train_Acc': 0.8709340184994862, 'val_Acc': 0.8087946603847664, 'test_Acc': 0.7297929191716767, 'class_Acc_0': nan, 'class_Acc_1': 0.08247422680412371, 'class_Acc_2': 0.8520833333333333, 'class_Acc_3': 0.1836734693877551, 'class_Acc_4': 0.7401247401247399, 'class_Acc_5': 0.8333333333333334, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.8107344632768362, 'class_Acc_8': 0.861271676300578, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-4842.6353, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc  val_bpd\n","766     -6903.96582   0.870934    -4842.635254  ...  0.729793  0.808795  5.81424\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  26%|                       | 130/500 [42:48<2:10:14, 21.12s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.787023918540257, 'Train_Acc': 0.8703522028091811, 'val_Acc': 0.8080094228504122, 'test_Acc': 0.7341349365397462, 'class_Acc_0': nan, 'class_Acc_1': 0.08247422680412371, 'class_Acc_2': 0.76875, 'class_Acc_3': 0.1326530612244898, 'class_Acc_4': 0.8253638253638252, 'class_Acc_5': 0.8809523809523809, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.8446327683615821, 'class_Acc_8': 0.8670520231213873, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-4882.8975, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","784    -7078.703125   0.870352    -4882.897461  ...  0.734135  0.808009  5.787024\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  27%|                       | 133/500 [43:57<2:09:12, 21.12s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.759813948057969, 'Train_Acc': 0.8750714354230902, 'val_Acc': 0.8076168040832352, 'test_Acc': 0.7358049432197729, 'class_Acc_0': nan, 'class_Acc_1': 0.11340206185567009, 'class_Acc_2': 0.7864583333333334, 'class_Acc_3': 0.14285714285714285, 'class_Acc_4': 0.8024948024948024, 'class_Acc_5': 0.8571428571428571, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.8644067796610171, 'class_Acc_8': 0.8583815028901735, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-4942.7886, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","802     -7118.48584   0.875071    -4942.788574  ...  0.735805  0.807617  5.759814\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  27%|                       | 136/500 [45:06<2:07:59, 21.10s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.7336808329561375, 'Train_Acc': 0.8728492360397396, 'val_Acc': 0.8158617981939537, 'test_Acc': 0.7247828991315965, 'class_Acc_0': nan, 'class_Acc_1': 0.061855670103092765, 'class_Acc_2': 0.8260416666666666, 'class_Acc_3': 0.14285714285714285, 'class_Acc_4': 0.7785862785862786, 'class_Acc_5': 0.8333333333333334, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.7909604519774012, 'class_Acc_8': 0.8265895953757226, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5007.266, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","820    -7176.776367   0.872849    -5007.266113  ...  0.724783  0.815862  5.733681\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  28%|                       | 139/500 [46:15<2:07:59, 21.27s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.722315617677156, 'Train_Acc': 0.8762195683453238, 'val_Acc': 0.8103651354534747, 'test_Acc': 0.7317969271877087, 'class_Acc_0': nan, 'class_Acc_1': 0.09278350515463916, 'class_Acc_2': 0.79375, 'class_Acc_3': 0.12244897959183673, 'class_Acc_4': 0.8014553014553013, 'class_Acc_5': 0.8809523809523809, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.824858757062147, 'class_Acc_8': 0.861271676300578, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5026.956, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","838     -7233.90625    0.87622    -5026.956055  ...  0.731797  0.810365  5.722316\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  28%|                       | 142/500 [47:24<2:05:26, 21.02s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.725293524335449, 'Train_Acc': 0.8802417677286741, 'val_Acc': 0.8115429917550059, 'test_Acc': 0.7244488977955912, 'class_Acc_0': nan, 'class_Acc_1': 0.13917525773195877, 'class_Acc_2': 0.8302083333333334, 'class_Acc_3': 0.11224489795918367, 'class_Acc_4': 0.7338877338877339, 'class_Acc_5': 0.8571428571428571, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.8361581920903955, 'class_Acc_8': 0.8526011560693642, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5591.283, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","857    -7664.351074   0.880242    -5591.283203  ...  0.724449  0.811543  5.725294\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  29%|                      | 146/500 [48:50<1:56:25, 19.73s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.668431756266349, 'Train_Acc': 0.8719344021925317, 'val_Acc': 0.806438947781704, 'test_Acc': 0.7234468937875751, 'class_Acc_0': nan, 'class_Acc_1': 0.03608247422680412, 'class_Acc_2': 0.7833333333333334, 'class_Acc_3': 0.1326530612244898, 'class_Acc_4': 0.8056133056133056, 'class_Acc_5': 0.8809523809523809, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.8135593220338982, 'class_Acc_8': 0.846820809248555, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5390.176, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","876    -7575.924805   0.871934    -5390.175781  ...  0.723447  0.806439  5.668432\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  30%|                      | 149/500 [49:59<2:00:33, 20.61s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.659451778003212, 'Train_Acc': 0.8803669338814663, 'val_Acc': 0.8138987043580683, 'test_Acc': 0.7171008684034736, 'class_Acc_0': nan, 'class_Acc_1': 0.15463917525773194, 'class_Acc_2': 0.8072916666666666, 'class_Acc_3': 0.1326530612244898, 'class_Acc_4': 0.7453222453222452, 'class_Acc_5': 0.8571428571428571, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.8022598870056499, 'class_Acc_8': 0.8410404624277455, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5286.236, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","895    -7581.488281   0.880367     -5286.23584  ...  0.717101  0.813899  5.659452\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  30%|                      | 152/500 [51:08<2:01:47, 21.00s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.658482467272615, 'Train_Acc': 0.8782033025008565, 'val_Acc': 0.8099725166862976, 'test_Acc': 0.7160988643954576, 'class_Acc_0': nan, 'class_Acc_1': 0.13917525773195877, 'class_Acc_2': 0.8145833333333334, 'class_Acc_3': 0.1326530612244898, 'class_Acc_4': 0.7307692307692307, 'class_Acc_5': 0.8333333333333334, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.8135593220338982, 'class_Acc_8': 0.8497109826589596, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-4834.6533, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","914    -7287.962891   0.878203     -4834.65332  ...  0.716099  0.809973  5.658482\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  31%|                      | 155/500 [52:17<2:00:33, 20.97s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.632585574698636, 'Train_Acc': 0.876790668036999, 'val_Acc': 0.806831566548881, 'test_Acc': 0.7201068804275217, 'class_Acc_0': nan, 'class_Acc_1': 0.12371134020618553, 'class_Acc_2': 0.7708333333333334, 'class_Acc_3': 0.12244897959183673, 'class_Acc_4': 0.7910602910602911, 'class_Acc_5': 0.8571428571428571, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.8192090395480226, 'class_Acc_8': 0.8439306358381503, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5184.3555, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","933    -7508.217773   0.876791    -5184.355469  ...  0.720107  0.806832  5.632586\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  32%|                      | 158/500 [53:26<2:00:20, 21.11s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.603830530425814, 'Train_Acc': 0.8793280849606029, 'val_Acc': 0.8154691794267765, 'test_Acc': 0.7107548430193721, 'class_Acc_0': nan, 'class_Acc_1': 0.14948453608247422, 'class_Acc_2': 0.8125, 'class_Acc_3': 0.10204081632653061, 'class_Acc_4': 0.7245322245322244, 'class_Acc_5': 0.8571428571428571, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.8022598870056499, 'class_Acc_8': 0.838150289017341, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5281.1235, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","952    -7657.501953   0.879328    -5281.123535  ...  0.710755  0.815469  5.603831\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  32%|                     | 161/500 [54:34<1:57:58, 20.88s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.603882893868867, 'Train_Acc': 0.8798033025008565, 'val_Acc': 0.8127208480565371, 'test_Acc': 0.7047428189712759, 'class_Acc_0': nan, 'class_Acc_1': 0.14432989690721648, 'class_Acc_2': 0.8114583333333335, 'class_Acc_3': 0.12244897959183673, 'class_Acc_4': 0.7162162162162162, 'class_Acc_5': 0.8571428571428571, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.7796610169491527, 'class_Acc_8': 0.8352601156069365, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5845.74, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","971    -8074.442383   0.879803    -5845.740234  ...  0.704743  0.812721  5.603883\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  33%|                     | 165/500 [56:00<1:50:13, 19.74s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.568329055290397, 'Train_Acc': 0.8818243508050703, 'val_Acc': 0.8111503729878288, 'test_Acc': 0.6940547762191048, 'class_Acc_0': nan, 'class_Acc_1': 0.1288659793814433, 'class_Acc_2': 0.796875, 'class_Acc_3': 0.1326530612244898, 'class_Acc_4': 0.7027027027027026, 'class_Acc_5': 0.8095238095238095, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.7740112994350283, 'class_Acc_8': 0.838150289017341, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5601.8975, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","     Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc  val_Acc   val_bpd\n","990    -7945.350098   0.881824    -5601.897461  ...  0.694055  0.81115  5.568329\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  34%|                     | 168/500 [57:10<1:54:16, 20.65s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.552766076462266, 'Train_Acc': 0.88610798218568, 'val_Acc': 0.8146839418924224, 'test_Acc': 0.6997327989311958, 'class_Acc_0': nan, 'class_Acc_1': 0.14948453608247422, 'class_Acc_2': 0.771875, 'class_Acc_3': 0.14285714285714285, 'class_Acc_4': 0.7432432432432431, 'class_Acc_5': 0.8571428571428571, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.7514124293785311, 'class_Acc_8': 0.846820809248555, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5525.986, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1009    -7915.007812   0.886108     -5525.98584  ...  0.699733  0.814684  5.552766\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  34%|                     | 171/500 [58:19<1:54:27, 20.87s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.537738472746964, 'Train_Acc': 0.8872176498800959, 'val_Acc': 0.81232822928936, 'test_Acc': 0.7030728122912492, 'class_Acc_0': nan, 'class_Acc_1': 0.16494845360824742, 'class_Acc_2': 0.8104166666666666, 'class_Acc_3': 0.15306122448979592, 'class_Acc_4': 0.7193347193347194, 'class_Acc_5': 0.8571428571428571, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.7429378531073447, 'class_Acc_8': 0.8294797687861272, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5088.1543, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1028    -7689.856934   0.887218    -5088.154297  ...  0.703073  0.812328  5.537738\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  35%|                    | 174/500 [59:28<1:53:57, 20.97s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.569100418117435, 'Train_Acc': 0.8859824323398425, 'val_Acc': 0.811935610522183, 'test_Acc': 0.7034068136272545, 'class_Acc_0': nan, 'class_Acc_1': 0.14432989690721648, 'class_Acc_2': 0.79375, 'class_Acc_3': 0.1326530612244898, 'class_Acc_4': 0.737006237006237, 'class_Acc_5': 0.8333333333333334, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.7655367231638419, 'class_Acc_8': 0.8265895953757226, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5339.223, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc  val_bpd\n","1047    -7783.161133   0.885982    -5339.223145  ...  0.703407  0.811936   5.5691\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  35%|                   | 177/500 [1:00:37<1:53:04, 21.00s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.539462474535735, 'Train_Acc': 0.8863554504967455, 'val_Acc': 0.8150765606595995, 'test_Acc': 0.7027388109552438, 'class_Acc_0': nan, 'class_Acc_1': 0.15979381443298968, 'class_Acc_2': 0.83125, 'class_Acc_3': 0.17346938775510204, 'class_Acc_4': 0.7047817047817049, 'class_Acc_5': 0.8095238095238095, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.7175141242937852, 'class_Acc_8': 0.838150289017341, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5423.848, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1066     -7939.98877   0.886355    -5423.848145  ...  0.702739  0.815077  5.539462\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  36%|                   | 180/500 [1:01:46<1:52:36, 21.12s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.50368415411171, 'Train_Acc': 0.8879372661870504, 'val_Acc': 0.811935610522183, 'test_Acc': 0.6953907815631263, 'class_Acc_0': nan, 'class_Acc_1': 0.17010309278350516, 'class_Acc_2': 0.7958333333333335, 'class_Acc_3': 0.1326530612244898, 'class_Acc_4': 0.7151767151767151, 'class_Acc_5': 0.8333333333333334, 'class_Acc_6': 0.10526315789473685, 'class_Acc_7': 0.7175141242937852, 'class_Acc_8': 0.8410404624277455, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6062.214, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1085    -8426.572266   0.887937    -6062.213867  ...  0.695391  0.811936  5.503684\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  37%|                   | 184/500 [1:03:12<1:44:57, 19.93s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.461974206176634, 'Train_Acc': 0.8835154504967455, 'val_Acc': 0.8095798979191206, 'test_Acc': 0.6910487641950568, 'class_Acc_0': nan, 'class_Acc_1': 0.11855670103092782, 'class_Acc_2': 0.803125, 'class_Acc_3': 0.11224489795918367, 'class_Acc_4': 0.6995841995841995, 'class_Acc_5': 0.8095238095238095, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.7655367231638419, 'class_Acc_8': 0.8208092485549134, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5820.892, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc  val_Acc   val_bpd\n","1104    -8273.722656   0.883515     -5820.89209  ...  0.691049  0.80958  5.461974\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  37%|                  | 187/500 [1:04:21<1:48:14, 20.75s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.4635479272319785, 'Train_Acc': 0.8848988009592326, 'val_Acc': 0.8084020416175893, 'test_Acc': 0.7044088176352705, 'class_Acc_0': nan, 'class_Acc_1': 0.11340206185567009, 'class_Acc_2': 0.7666666666666665, 'class_Acc_3': 0.15306122448979592, 'class_Acc_4': 0.7702702702702701, 'class_Acc_5': 0.8333333333333334, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.7824858757062148, 'class_Acc_8': 0.8121387283236995, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5706.999, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1123    -8239.264648   0.884899    -5706.999023  ...  0.704409  0.808402  5.463548\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  38%|                  | 190/500 [1:05:30<1:48:06, 20.92s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.44719832268982, 'Train_Acc': 0.8876354504967454, 'val_Acc': 0.8115429917550059, 'test_Acc': 0.6963927855711423, 'class_Acc_0': nan, 'class_Acc_1': 0.1288659793814433, 'class_Acc_2': 0.7770833333333333, 'class_Acc_3': 0.14285714285714285, 'class_Acc_4': 0.7432432432432431, 'class_Acc_5': 0.8333333333333334, 'class_Acc_6': 0.07894736842105263, 'class_Acc_7': 0.7485875706214689, 'class_Acc_8': 0.815028901734104, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5277.4316, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1142    -7996.637207   0.887635    -5277.431641  ...  0.696393  0.811543  5.447198\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  39%|                  | 193/500 [1:06:38<1:47:14, 20.96s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.457494901065957, 'Train_Acc': 0.8882354504967455, 'val_Acc': 0.8103651354534747, 'test_Acc': 0.6973947895791583, 'class_Acc_0': nan, 'class_Acc_1': 0.13917525773195877, 'class_Acc_2': 0.7708333333333334, 'class_Acc_3': 0.11224489795918367, 'class_Acc_4': 0.7474012474012474, 'class_Acc_5': 0.8571428571428571, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.788135593220339, 'class_Acc_8': 0.7919075144508669, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5568.723, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1161    -8157.116211   0.888235    -5568.723145  ...  0.697395  0.810365  5.457495\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  39%|                  | 196/500 [1:07:47<1:46:13, 20.97s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.424065656356296, 'Train_Acc': 0.894410565262076, 'val_Acc': 0.8150765606595995, 'test_Acc': 0.6880427521710086, 'class_Acc_0': nan, 'class_Acc_1': 0.2422680412371134, 'class_Acc_2': 0.7927083333333336, 'class_Acc_3': 0.14285714285714285, 'class_Acc_4': 0.6923076923076924, 'class_Acc_5': 0.8333333333333334, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.7401129943502824, 'class_Acc_8': 0.7890173410404624, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5666.0493, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1180    -8309.360352   0.894411    -5666.049316  ...  0.688043  0.815077  5.424066\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  40%|                  | 199/500 [1:08:56<1:44:40, 20.86s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.416358368054359, 'Train_Acc': 0.8900239671120247, 'val_Acc': 0.8127208480565371, 'test_Acc': 0.6910487641950568, 'class_Acc_0': nan, 'class_Acc_1': 0.1804123711340206, 'class_Acc_2': 0.7802083333333335, 'class_Acc_3': 0.11224489795918367, 'class_Acc_4': 0.7141372141372141, 'class_Acc_5': 0.8095238095238095, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.7570621468926554, 'class_Acc_8': 0.8208092485549134, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6247.8022, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1199    -8717.030273   0.890024    -6247.802246  ...  0.691049  0.812721  5.416358\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  41%|                 | 203/500 [1:10:22<1:37:38, 19.73s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.403578635372005, 'Train_Acc': 0.8954050154162384, 'val_Acc': 0.8162544169611308, 'test_Acc': 0.6843687374749499, 'class_Acc_0': nan, 'class_Acc_1': 0.2422680412371134, 'class_Acc_2': 0.7645833333333335, 'class_Acc_3': 0.1326530612244898, 'class_Acc_4': 0.6777546777546777, 'class_Acc_5': 0.8571428571428571, 'class_Acc_6': 0.10526315789473685, 'class_Acc_7': 0.7768361581920906, 'class_Acc_8': 0.8323699421965317, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5852.837, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1219    -8432.206055   0.895405    -5852.836914  ...  0.684369  0.816254  5.403579\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  41%|                 | 206/500 [1:11:31<1:41:13, 20.66s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.385080720782795, 'Train_Acc': 0.8933550668036999, 'val_Acc': 0.8158617981939537, 'test_Acc': 0.6920507682030728, 'class_Acc_0': nan, 'class_Acc_1': 0.19072164948453607, 'class_Acc_2': 0.7895833333333334, 'class_Acc_3': 0.15306122448979592, 'class_Acc_4': 0.712058212058212, 'class_Acc_5': 0.8571428571428571, 'class_Acc_6': 0.07894736842105263, 'class_Acc_7': 0.7203389830508475, 'class_Acc_8': 0.8179190751445087, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5419.0503, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1238    -8197.208008   0.893355    -5419.050293  ...  0.692051  0.815862  5.385081\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  42%|                 | 209/500 [1:12:40<1:41:13, 20.87s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.376306204459889, 'Train_Acc': 0.8928869338814662, 'val_Acc': 0.817432273262662, 'test_Acc': 0.6907147628590514, 'class_Acc_0': nan, 'class_Acc_1': 0.15979381443298968, 'class_Acc_2': 0.7843749999999999, 'class_Acc_3': 0.1326530612244898, 'class_Acc_4': 0.7224532224532225, 'class_Acc_5': 0.8333333333333334, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.7627118644067798, 'class_Acc_8': 0.777456647398844, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5768.7197, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1258    -8444.829102   0.892887    -5768.719727  ...  0.690715  0.817432  5.376306\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  43%|                 | 213/500 [1:14:06<1:34:07, 19.68s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.3382248317961105, 'Train_Acc': 0.8926857828023297, 'val_Acc': 0.81232822928936, 'test_Acc': 0.6793587174348698, 'class_Acc_0': nan, 'class_Acc_1': 0.14948453608247422, 'class_Acc_2': 0.7885416666666667, 'class_Acc_3': 0.14285714285714285, 'class_Acc_4': 0.687110187110187, 'class_Acc_5': 0.8095238095238095, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.7372881355932204, 'class_Acc_8': 0.7976878612716762, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6088.8784, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1278    -8699.453125   0.892686    -6088.878418  ...  0.679359  0.812328  5.338225\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  43%|                 | 216/500 [1:15:15<1:37:22, 20.57s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.327973807267294, 'Train_Acc': 0.8918988009592326, 'val_Acc': 0.811935610522183, 'test_Acc': 0.6823647294589178, 'class_Acc_0': nan, 'class_Acc_1': 0.13402061855670103, 'class_Acc_2': 0.7770833333333333, 'class_Acc_3': 0.15306122448979592, 'class_Acc_4': 0.7234927234927234, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.7203389830508475, 'class_Acc_8': 0.7803468208092487, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6012.8145, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1297    -8668.785156   0.891899    -6012.814453  ...  0.682365  0.811936  5.327974\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  44%|                | 219/500 [1:16:24<1:38:19, 20.99s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.3365919377019795, 'Train_Acc': 0.8957931483384721, 'val_Acc': 0.8162544169611308, 'test_Acc': 0.6833667334669339, 'class_Acc_0': nan, 'class_Acc_1': 0.19587628865979378, 'class_Acc_2': 0.79375, 'class_Acc_3': 0.163265306122449, 'class_Acc_4': 0.6860706860706861, 'class_Acc_5': 0.8571428571428571, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.7231638418079097, 'class_Acc_8': 0.7976878612716762, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5505.247, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1316    -8381.521484   0.895793     -5505.24707  ...  0.683367  0.816254  5.336592\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  44%|                | 222/500 [1:17:34<1:38:13, 21.20s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.29477186216694, 'Train_Acc': 0.8955091332648167, 'val_Acc': 0.8146839418924224, 'test_Acc': 0.6800267201068805, 'class_Acc_0': nan, 'class_Acc_1': 0.15463917525773194, 'class_Acc_2': 0.7520833333333334, 'class_Acc_3': 0.15306122448979592, 'class_Acc_4': 0.7214137214137214, 'class_Acc_5': 0.8571428571428571, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.7401129943502824, 'class_Acc_8': 0.7976878612716762, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5910.7896, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1335    -8658.457031   0.895509    -5910.789551  ...  0.680027  0.814684  5.294772\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  45%|                | 225/500 [1:18:43<1:36:37, 21.08s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.2964841232734114, 'Train_Acc': 0.8932061664953752, 'val_Acc': 0.8111503729878288, 'test_Acc': 0.6780227120908484, 'class_Acc_0': nan, 'class_Acc_1': 0.13917525773195877, 'class_Acc_2': 0.7416666666666667, 'class_Acc_3': 0.14285714285714285, 'class_Acc_4': 0.737006237006237, 'class_Acc_5': 0.8095238095238095, 'class_Acc_6': 0.0, 'class_Acc_7': 0.7514124293785311, 'class_Acc_8': 0.7745664739884393, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6508.0977, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc  val_Acc   val_bpd\n","1355    -9103.259766   0.893206    -6508.097656  ...  0.678023  0.81115  5.296484\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  46%|                | 229/500 [1:20:09<1:29:24, 19.80s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.28006524870569, 'Train_Acc': 0.8973127646454265, 'val_Acc': 0.8111503729878288, 'test_Acc': 0.6713426853707415, 'class_Acc_0': nan, 'class_Acc_1': 0.15463917525773194, 'class_Acc_2': 0.759375, 'class_Acc_3': 0.14285714285714285, 'class_Acc_4': 0.6902286902286902, 'class_Acc_5': 0.8095238095238095, 'class_Acc_6': 0.07894736842105263, 'class_Acc_7': 0.7429378531073447, 'class_Acc_8': 0.7890173410404624, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6110.5874, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc  val_Acc   val_bpd\n","1375      -8902.4375   0.897313    -6110.587402  ...  0.671343  0.81115  5.280065\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  46%|                | 232/500 [1:21:18<1:32:52, 20.79s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.273791732526096, 'Train_Acc': 0.8996735320315177, 'val_Acc': 0.8182175107970161, 'test_Acc': 0.6710086840347361, 'class_Acc_0': nan, 'class_Acc_1': 0.22680412371134018, 'class_Acc_2': 0.7822916666666667, 'class_Acc_3': 0.12244897959183673, 'class_Acc_4': 0.6632016632016632, 'class_Acc_5': 0.8333333333333334, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.7259887005649718, 'class_Acc_8': 0.7803468208092487, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5638.3037, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1394    -8606.355469   0.899674    -5638.303711  ...  0.671009  0.818218  5.273792\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  47%|                | 235/500 [1:22:28<1:32:41, 20.99s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.245475554957606, 'Train_Acc': 0.8952917163412127, 'val_Acc': 0.81232822928936, 'test_Acc': 0.667000668002672, 'class_Acc_0': nan, 'class_Acc_1': 0.12371134020618553, 'class_Acc_2': 0.74375, 'class_Acc_3': 0.1326530612244898, 'class_Acc_4': 0.7089397089397089, 'class_Acc_5': 0.8571428571428571, 'class_Acc_6': 0.07894736842105263, 'class_Acc_7': 0.7033898305084746, 'class_Acc_8': 0.7976878612716762, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6040.465, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1414    -8852.547852   0.895292    -6040.464844  ...  0.667001  0.812328  5.245476\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  48%|               | 239/500 [1:23:55<1:26:54, 19.98s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.227274679630558, 'Train_Acc': 0.8938469338814663, 'val_Acc': 0.8103651354534747, 'test_Acc': 0.6646626586506346, 'class_Acc_0': nan, 'class_Acc_1': 0.09278350515463916, 'class_Acc_2': 0.721875, 'class_Acc_3': 0.1326530612244898, 'class_Acc_4': 0.7266112266112266, 'class_Acc_5': 0.8809523809523809, 'class_Acc_6': 0.07894736842105263, 'class_Acc_7': 0.7344632768361581, 'class_Acc_8': 0.7716763005780347, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6315.257, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1434     -9112.96875   0.893847    -6315.256836  ...  0.664663  0.810365  5.227275\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  48%|               | 242/500 [1:25:05<1:29:38, 20.85s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.225481173002578, 'Train_Acc': 0.900917547105173, 'val_Acc': 0.8131134668237142, 'test_Acc': 0.667000668002672, 'class_Acc_0': nan, 'class_Acc_1': 0.1804123711340206, 'class_Acc_2': 0.7239583333333334, 'class_Acc_3': 0.17346938775510204, 'class_Acc_4': 0.7245322245322244, 'class_Acc_5': 0.8809523809523809, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.7033898305084746, 'class_Acc_8': 0.7658959537572254, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5748.1943, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1454    -8671.103516   0.900918    -5748.194336  ...  0.667001  0.813113  5.225481\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  49%|               | 245/500 [1:26:13<1:28:49, 20.90s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.236502386290201, 'Train_Acc': 0.8978446317231928, 'val_Acc': 0.8107577542206518, 'test_Acc': 0.6549766199064796, 'class_Acc_0': nan, 'class_Acc_1': 0.12371134020618553, 'class_Acc_2': 0.7177083333333333, 'class_Acc_3': 0.163265306122449, 'class_Acc_4': 0.7006237006237006, 'class_Acc_5': 0.8095238095238095, 'class_Acc_6': 0.07894736842105263, 'class_Acc_7': 0.692090395480226, 'class_Acc_8': 0.7976878612716762, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6052.652, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1474       -8935.125   0.897845    -6052.651855  ...  0.654977  0.810758  5.236502\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  50%|               | 249/500 [1:27:41<1:23:43, 20.01s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.2077499249674855, 'Train_Acc': 0.8988097978759849, 'val_Acc': 0.8138987043580683, 'test_Acc': 0.6629926519706079, 'class_Acc_0': nan, 'class_Acc_1': 0.09793814432989689, 'class_Acc_2': 0.74375, 'class_Acc_3': 0.14285714285714285, 'class_Acc_4': 0.6933471933471933, 'class_Acc_5': 0.8333333333333334, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.7231638418079097, 'class_Acc_8': 0.8034682080924856, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6365.2856, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc  val_bpd\n","1494    -9177.371094    0.89881    -6365.285645  ...  0.662993  0.813899  5.20775\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  50%|               | 252/500 [1:28:50<1:26:11, 20.85s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.177496534819677, 'Train_Acc': 0.9014109489551215, 'val_Acc': 0.8162544169611308, 'test_Acc': 0.6609886439545758, 'class_Acc_0': nan, 'class_Acc_1': 0.15463917525773194, 'class_Acc_2': 0.7625000000000001, 'class_Acc_3': 0.17346938775510204, 'class_Acc_4': 0.6839916839916839, 'class_Acc_5': 0.8333333333333334, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.672316384180791, 'class_Acc_8': 0.7716763005780347, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5855.053, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1514    -8834.318359   0.901411    -5855.053223  ...  0.660989  0.816254  5.177497\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  51%|              | 255/500 [1:29:59<1:25:21, 20.91s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.190105323167759, 'Train_Acc': 0.9013364988009592, 'val_Acc': 0.8154691794267765, 'test_Acc': 0.6633266533066132, 'class_Acc_0': nan, 'class_Acc_1': 0.15463917525773194, 'class_Acc_2': 0.7479166666666666, 'class_Acc_3': 0.19387755102040816, 'class_Acc_4': 0.6673596673596672, 'class_Acc_5': 0.8095238095238095, 'class_Acc_6': 0.10526315789473685, 'class_Acc_7': 0.7740112994350283, 'class_Acc_8': 0.7658959537572254, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6170.83, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1534    -9017.441406   0.901336    -6170.830078  ...  0.663327  0.815469  5.190105\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  52%|              | 259/500 [1:31:25<1:19:48, 19.87s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.176144243032489, 'Train_Acc': 0.8988695169578623, 'val_Acc': 0.8127208480565371, 'test_Acc': 0.6629926519706079, 'class_Acc_0': nan, 'class_Acc_1': 0.10824742268041236, 'class_Acc_2': 0.7510416666666667, 'class_Acc_3': 0.11224489795918367, 'class_Acc_4': 0.7006237006237006, 'class_Acc_5': 0.8571428571428571, 'class_Acc_6': 0.07894736842105263, 'class_Acc_7': 0.6864406779661018, 'class_Acc_8': 0.7976878612716762, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6419.572, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1554    -9292.978516    0.89887    -6419.571777  ...  0.662993  0.812721  5.176144\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  52%|              | 262/500 [1:32:35<1:22:42, 20.85s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.163527940647737, 'Train_Acc': 0.9088494141829394, 'val_Acc': 0.8205732234000785, 'test_Acc': 0.6616566466265865, 'class_Acc_0': nan, 'class_Acc_1': 0.23195876288659792, 'class_Acc_2': 0.746875, 'class_Acc_3': 0.17346938775510204, 'class_Acc_4': 0.6725571725571725, 'class_Acc_5': 0.8333333333333334, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.7146892655367232, 'class_Acc_8': 0.7658959537572254, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5880.6816, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1574    -8953.415039   0.908849    -5880.681641  ...  0.661657  0.820573  5.163528\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  53%|              | 265/500 [1:33:44<1:21:54, 20.91s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.118291326563717, 'Train_Acc': 0.9020109489551216, 'val_Acc': 0.8162544169611308, 'test_Acc': 0.6492985971943888, 'class_Acc_0': nan, 'class_Acc_1': 0.13917525773195877, 'class_Acc_2': 0.7729166666666667, 'class_Acc_3': 0.163265306122449, 'class_Acc_4': 0.6621621621621622, 'class_Acc_5': 0.8095238095238095, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.6497175141242938, 'class_Acc_8': 0.7398843930635839, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6303.351, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1594    -9283.916992   0.902011    -6303.351074  ...  0.649299  0.816254  5.118291\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  54%|             | 269/500 [1:35:10<1:16:39, 19.91s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.1426694444755086, 'Train_Acc': 0.9034220486467969, 'val_Acc': 0.8135060855908912, 'test_Acc': 0.6569806279225117, 'class_Acc_0': nan, 'class_Acc_1': 0.15979381443298968, 'class_Acc_2': 0.7343750000000001, 'class_Acc_3': 0.163265306122449, 'class_Acc_4': 0.6912681912681913, 'class_Acc_5': 0.8333333333333334, 'class_Acc_6': 0.0, 'class_Acc_7': 0.6581920903954803, 'class_Acc_8': 0.815028901734104, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6499.2144, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1614    -9374.160156   0.903422    -6499.214355  ...  0.656981  0.813506  5.142669\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  54%|             | 272/500 [1:36:19<1:18:43, 20.72s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.118938472972032, 'Train_Acc': 0.9097916135662899, 'val_Acc': 0.8193953670985473, 'test_Acc': 0.6503006012024048, 'class_Acc_0': nan, 'class_Acc_1': 0.21649484536082472, 'class_Acc_2': 0.7604166666666666, 'class_Acc_3': 0.17346938775510204, 'class_Acc_4': 0.6444906444906444, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.672316384180791, 'class_Acc_8': 0.7687861271676301, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5971.0024, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1634    -9072.115234   0.909792    -5971.002441  ...  0.650301  0.819395  5.118938\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  55%|             | 275/500 [1:37:29<1:18:59, 21.07s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.131303289941942, 'Train_Acc': 0.9047856800274067, 'val_Acc': 0.8142913231252454, 'test_Acc': 0.6526386105544422, 'class_Acc_0': nan, 'class_Acc_1': 0.09793814432989689, 'class_Acc_2': 0.7635416666666668, 'class_Acc_3': 0.19387755102040816, 'class_Acc_4': 0.6954261954261953, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.07894736842105263, 'class_Acc_7': 0.632768361581921, 'class_Acc_8': 0.7341040462427746, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6296.62, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1654    -9283.248047   0.904786    -6296.620117  ...  0.652639  0.814291  5.131303\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  56%|             | 279/500 [1:38:55<1:13:12, 19.88s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.10296222212698, 'Train_Acc': 0.9066842480301472, 'val_Acc': 0.8170396544954849, 'test_Acc': 0.64562458249833, 'class_Acc_0': nan, 'class_Acc_1': 0.14432989690721648, 'class_Acc_2': 0.7822916666666667, 'class_Acc_3': 0.20408163265306123, 'class_Acc_4': 0.6496881496881497, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6468926553672316, 'class_Acc_8': 0.7109826589595376, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6598.127, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc  val_Acc   val_bpd\n","1674    -9560.791992   0.906684    -6598.126953  ...  0.645625  0.81704  5.102962\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  56%|             | 282/500 [1:40:03<1:14:17, 20.45s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.115721760835779, 'Train_Acc': 0.91198452894827, 'val_Acc': 0.8186101295641932, 'test_Acc': 0.648630594522378, 'class_Acc_0': nan, 'class_Acc_1': 0.20618556701030927, 'class_Acc_2': 0.7791666666666666, 'class_Acc_3': 0.24489795918367346, 'class_Acc_4': 0.6413721413721413, 'class_Acc_5': 0.8095238095238095, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.652542372881356, 'class_Acc_8': 0.7109826589595376, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-5985.2017, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc  val_Acc   val_bpd\n","1694    -9136.860352   0.911985     -5985.20166  ...  0.648631  0.81861  5.115722\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  57%|             | 285/500 [1:41:12<1:14:37, 20.83s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.115514655200026, 'Train_Acc': 0.9125456800274067, 'val_Acc': 0.8201806046329014, 'test_Acc': 0.6539746158984636, 'class_Acc_0': nan, 'class_Acc_1': 0.24742268041237106, 'class_Acc_2': 0.759375, 'class_Acc_3': 0.15306122448979592, 'class_Acc_4': 0.6444906444906444, 'class_Acc_5': 0.8095238095238095, 'class_Acc_6': 0.07894736842105263, 'class_Acc_7': 0.6949152542372882, 'class_Acc_8': 0.7601156069364162, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6306.9517, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1714    -9349.195312   0.912546     -6306.95166  ...  0.653975  0.820181  5.115515\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  58%|            | 289/500 [1:42:39<1:09:53, 19.88s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.084098233063689, 'Train_Acc': 0.9076438643371016, 'val_Acc': 0.8162544169611308, 'test_Acc': 0.6499665998663995, 'class_Acc_0': nan, 'class_Acc_1': 0.15979381443298968, 'class_Acc_2': 0.7229166666666667, 'class_Acc_3': 0.20408163265306123, 'class_Acc_4': 0.6829521829521829, 'class_Acc_5': 0.8095238095238095, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.7033898305084746, 'class_Acc_8': 0.7514450867052024, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6632.9307, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1734    -9588.645508   0.907644    -6632.930664  ...  0.649967  0.816254  5.084098\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  58%|            | 292/500 [1:43:48<1:11:37, 20.66s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.079015456882848, 'Train_Acc': 0.9172138129496402, 'val_Acc': 0.8205732234000785, 'test_Acc': 0.6519706078824316, 'class_Acc_0': nan, 'class_Acc_1': 0.26804123711340205, 'class_Acc_2': 0.7572916666666667, 'class_Acc_3': 0.2346938775510204, 'class_Acc_4': 0.6476091476091476, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.6610169491525424, 'class_Acc_8': 0.7456647398843932, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6060.985, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1754    -9258.734375   0.917214    -6060.984863  ...  0.651971  0.820573  5.079015\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  59%|            | 295/500 [1:44:57<1:11:22, 20.89s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.043666610617318, 'Train_Acc': 0.9146134292565947, 'val_Acc': 0.8221436984687868, 'test_Acc': 0.6492985971943888, 'class_Acc_0': nan, 'class_Acc_1': 0.19072164948453607, 'class_Acc_2': 0.7385416666666668, 'class_Acc_3': 0.21428571428571427, 'class_Acc_4': 0.6569646569646569, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.6892655367231639, 'class_Acc_8': 0.7687861271676301, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6463.662, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1774    -9560.086914   0.914613    -6463.662109  ...  0.649299  0.822144  5.043667\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  60%|            | 299/500 [1:46:23<1:05:58, 19.69s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.032713962914318, 'Train_Acc': 0.9101345803357315, 'val_Acc': 0.8154691794267765, 'test_Acc': 0.6449565798263193, 'class_Acc_0': nan, 'class_Acc_1': 0.1804123711340206, 'class_Acc_2': 0.7166666666666666, 'class_Acc_3': 0.22448979591836735, 'class_Acc_4': 0.685031185031185, 'class_Acc_5': 0.8095238095238095, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6553672316384181, 'class_Acc_8': 0.7514450867052024, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6726.3086, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1794    -9744.787109   0.910135    -6726.308594  ...  0.644957  0.815469  5.032714\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  60%|            | 302/500 [1:47:33<1:08:50, 20.86s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.031645795638765, 'Train_Acc': 0.9190989791024323, 'val_Acc': 0.8237141735374951, 'test_Acc': 0.648630594522378, 'class_Acc_0': nan, 'class_Acc_1': 0.22680412371134018, 'class_Acc_2': 0.7374999999999999, 'class_Acc_3': 0.24489795918367346, 'class_Acc_4': 0.6735966735966735, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6355932203389831, 'class_Acc_8': 0.7485549132947976, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6170.379, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1814    -9405.539062   0.919099    -6170.378906  ...  0.648631  0.823714  5.031646\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  61%|           | 305/500 [1:48:43<1:08:38, 21.12s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.010497304613754, 'Train_Acc': 0.9132082631038028, 'val_Acc': 0.8197879858657244, 'test_Acc': 0.6352705410821643, 'class_Acc_0': nan, 'class_Acc_1': 0.1752577319587629, 'class_Acc_2': 0.7125, 'class_Acc_3': 0.22448979591836735, 'class_Acc_4': 0.6819126819126818, 'class_Acc_5': 0.761904761904762, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6271186440677967, 'class_Acc_8': 0.7254335260115607, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6528.1904, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1834     -9671.96875   0.913208     -6528.19043  ...  0.635271  0.819788  5.010497\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  62%|           | 309/500 [1:50:09<1:03:36, 19.98s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.0051475453174366, 'Train_Acc': 0.914100130181569, 'val_Acc': 0.8186101295641932, 'test_Acc': 0.6402805611222445, 'class_Acc_0': nan, 'class_Acc_1': 0.1752577319587629, 'class_Acc_2': 0.7052083333333334, 'class_Acc_3': 0.20408163265306123, 'class_Acc_4': 0.6902286902286902, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6553672316384181, 'class_Acc_8': 0.7398843930635839, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6781.963, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc  val_Acc   val_bpd\n","1854    -9920.117188     0.9141    -6781.962891  ...  0.640281  0.81861  5.005148\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  62%|           | 312/500 [1:51:18<1:04:59, 20.74s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.995458899470738, 'Train_Acc': 0.9148719972593354, 'val_Acc': 0.8201806046329014, 'test_Acc': 0.6295925183700735, 'class_Acc_0': nan, 'class_Acc_1': 0.15463917525773194, 'class_Acc_2': 0.7020833333333335, 'class_Acc_3': 0.24489795918367346, 'class_Acc_4': 0.6642411642411642, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6553672316384181, 'class_Acc_8': 0.7283236994219655, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6240.3794, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1874    -9495.132812   0.914872    -6240.379395  ...  0.629593  0.820181  4.995459\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  63%|           | 315/500 [1:52:28<1:05:15, 21.16s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 5.008499979741065, 'Train_Acc': 0.919367112024666, 'val_Acc': 0.8209658421672555, 'test_Acc': 0.6482965931863728, 'class_Acc_0': nan, 'class_Acc_1': 0.20103092783505153, 'class_Acc_2': 0.7177083333333333, 'class_Acc_3': 0.24489795918367346, 'class_Acc_4': 0.687110187110187, 'class_Acc_5': 0.8333333333333334, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6553672316384181, 'class_Acc_8': 0.7514450867052024, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6512.7837, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc  val_bpd\n","1893    -9653.075195   0.919367    -6512.783691  ...  0.648297  0.820966   5.0085\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  64%|           | 318/500 [1:53:37<1:04:02, 21.11s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.99094578119504, 'Train_Acc': 0.921247112024666, 'val_Acc': 0.8225363172359639, 'test_Acc': 0.6379425517702071, 'class_Acc_0': nan, 'class_Acc_1': 0.22680412371134018, 'class_Acc_2': 0.7125, 'class_Acc_3': 0.22448979591836735, 'class_Acc_4': 0.661122661122661, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6581920903954803, 'class_Acc_8': 0.7427745664739884, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7143.179, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1913    -10183.49707   0.921247    -7143.179199  ...  0.637943  0.822536  4.990946\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  64%|           | 322/500 [1:55:02<58:56, 19.87s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.984783566683639, 'Train_Acc': 0.9169734292565946, 'val_Acc': 0.8209658421672555, 'test_Acc': 0.6299265197060788, 'class_Acc_0': nan, 'class_Acc_1': 0.1752577319587629, 'class_Acc_2': 0.721875, 'class_Acc_3': 0.22448979591836735, 'class_Acc_4': 0.6663201663201662, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6129943502824858, 'class_Acc_8': 0.7080924855491331, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6726.3677, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1933    -9780.085938   0.916973    -6726.367676  ...  0.629927  0.820966  4.984784\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  65%|          | 325/500 [1:56:10<1:00:09, 20.63s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.985820268930632, 'Train_Acc': 0.9216815621788285, 'val_Acc': 0.8237141735374951, 'test_Acc': 0.6336005344021376, 'class_Acc_0': nan, 'class_Acc_1': 0.2422680412371134, 'class_Acc_2': 0.7125, 'class_Acc_3': 0.25510204081632654, 'class_Acc_4': 0.6528066528066527, 'class_Acc_5': 0.8095238095238095, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6355932203389831, 'class_Acc_8': 0.73121387283237, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6557.492, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc  val_bpd\n","1953    -9661.030273   0.921682    -6557.492188  ...  0.633601  0.823714  4.98582\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  66%|           | 328/500 [1:57:19<59:52, 20.89s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.970684650938181, 'Train_Acc': 0.9208063446385748, 'val_Acc': 0.8209658421672555, 'test_Acc': 0.6279225116900468, 'class_Acc_0': nan, 'class_Acc_1': 0.19587628865979378, 'class_Acc_2': 0.7374999999999999, 'class_Acc_3': 0.2346938775510204, 'class_Acc_4': 0.6226611226611226, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6299435028248588, 'class_Acc_8': 0.7369942196531793, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7196.664, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1973   -10261.548828   0.920806    -7196.664062  ...  0.627923  0.820966  4.970685\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  66%|          | 332/500 [1:58:45<55:55, 19.98s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.93381491668765, 'Train_Acc': 0.919309695101062, 'val_Acc': 0.822928936003141, 'test_Acc': 0.6362725450901804, 'class_Acc_0': nan, 'class_Acc_1': 0.21649484536082472, 'class_Acc_2': 0.7322916666666666, 'class_Acc_3': 0.24489795918367346, 'class_Acc_4': 0.659043659043659, 'class_Acc_5': 0.8095238095238095, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6299435028248588, 'class_Acc_8': 0.7052023121387284, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6832.923, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","1993    -10017.40918    0.91931    -6832.922852  ...  0.636273  0.822929  4.933815\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  67%|          | 335/500 [1:59:54<56:50, 20.67s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.964048817302841, 'Train_Acc': 0.9237778280232958, 'val_Acc': 0.8252846486062034, 'test_Acc': 0.6275885103540414, 'class_Acc_0': nan, 'class_Acc_1': 0.2577319587628866, 'class_Acc_2': 0.7197916666666667, 'class_Acc_3': 0.29591836734693877, 'class_Acc_4': 0.5966735966735965, 'class_Acc_5': 0.8095238095238095, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6779661016949152, 'class_Acc_8': 0.7514450867052024, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6618.3086, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2013    -9839.447266   0.923778    -6618.308594  ...  0.627589  0.825285  4.964049\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  68%|          | 338/500 [2:01:03<56:28, 20.92s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.932159245669858, 'Train_Acc': 0.9141912298732443, 'val_Acc': 0.8201806046329014, 'test_Acc': 0.6295925183700735, 'class_Acc_0': nan, 'class_Acc_1': 0.17010309278350516, 'class_Acc_2': 0.7260416666666667, 'class_Acc_3': 0.20408163265306123, 'class_Acc_4': 0.6528066528066527, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.615819209039548, 'class_Acc_8': 0.7369942196531793, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7284.353, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2033   -10364.730469   0.914191    -7284.353027  ...  0.629593  0.820181  4.932159\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  68%|          | 342/500 [2:02:30<52:46, 20.04s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.928646433526027, 'Train_Acc': 0.9222867283316204, 'val_Acc': 0.822928936003141, 'test_Acc': 0.6356045424181697, 'class_Acc_0': nan, 'class_Acc_1': 0.20618556701030927, 'class_Acc_2': 0.7125, 'class_Acc_3': 0.21428571428571427, 'class_Acc_4': 0.6652806652806652, 'class_Acc_5': 0.8095238095238095, 'class_Acc_6': 0.0, 'class_Acc_7': 0.6638418079096046, 'class_Acc_8': 0.7196531791907514, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6846.091, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2053   -10072.496094   0.922287     -6846.09082  ...  0.635605  0.822929  4.928646\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  69%|          | 345/500 [2:03:38<53:21, 20.65s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.919865107607387, 'Train_Acc': 0.9234800274066461, 'val_Acc': 0.8217510797016098, 'test_Acc': 0.6275885103540414, 'class_Acc_0': nan, 'class_Acc_1': 0.23711340206185563, 'class_Acc_2': 0.71875, 'class_Acc_3': 0.24489795918367346, 'class_Acc_4': 0.6330561330561331, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6610169491525424, 'class_Acc_8': 0.6994219653179191, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6712.3496, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2073    -9938.614258    0.92348    -6712.349609  ...  0.627589  0.821751  4.919865\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  70%|         | 348/500 [2:04:47<52:56, 20.90s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.907233777149291, 'Train_Acc': 0.9208763960260362, 'val_Acc': 0.8201806046329014, 'test_Acc': 0.625250501002004, 'class_Acc_0': nan, 'class_Acc_1': 0.19072164948453607, 'class_Acc_2': 0.7343750000000001, 'class_Acc_3': 0.25510204081632654, 'class_Acc_4': 0.6288981288981288, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6384180790960452, 'class_Acc_8': 0.6936416184971098, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7324.254, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2093   -10426.818359   0.920876    -7324.253906  ...  0.625251  0.820181  4.907234\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  70%|         | 352/500 [2:06:12<48:46, 19.77s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.913283750737915, 'Train_Acc': 0.9178534292565946, 'val_Acc': 0.8201806046329014, 'test_Acc': 0.6265865063460254, 'class_Acc_0': nan, 'class_Acc_1': 0.16494845360824742, 'class_Acc_2': 0.7239583333333334, 'class_Acc_3': 0.21428571428571427, 'class_Acc_4': 0.6559251559251559, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6440677966101696, 'class_Acc_8': 0.6791907514450868, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6870.1606, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2113   -10138.734375   0.917853    -6870.160645  ...  0.626587  0.820181  4.913284\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  71%|         | 355/500 [2:07:21<49:58, 20.68s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.904932133791415, 'Train_Acc': 0.9257826104830421, 'val_Acc': 0.8237141735374951, 'test_Acc': 0.6259185036740147, 'class_Acc_0': nan, 'class_Acc_1': 0.2422680412371134, 'class_Acc_2': 0.709375, 'class_Acc_3': 0.24489795918367346, 'class_Acc_4': 0.6392931392931392, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6666666666666667, 'class_Acc_8': 0.684971098265896, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6738.415, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2133    -10036.71875   0.925783    -6738.415039  ...  0.625919  0.823714  4.904932\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  72%|         | 358/500 [2:08:30<49:06, 20.75s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.891236393076101, 'Train_Acc': 0.925754093867763, 'val_Acc': 0.8237141735374951, 'test_Acc': 0.6285905143620575, 'class_Acc_0': nan, 'class_Acc_1': 0.22164948453608246, 'class_Acc_2': 0.728125, 'class_Acc_3': 0.2346938775510204, 'class_Acc_4': 0.6247401247401246, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.672316384180791, 'class_Acc_8': 0.7052023121387284, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7356.078, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2153   -10535.726562   0.925754    -7356.078125  ...  0.628591  0.823714  4.891236\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  72%|        | 362/500 [2:09:55<45:17, 19.69s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.885040600213324, 'Train_Acc': 0.9215778280232957, 'val_Acc': 0.8221436984687868, 'test_Acc': 0.6282565130260521, 'class_Acc_0': nan, 'class_Acc_1': 0.20103092783505153, 'class_Acc_2': 0.6895833333333334, 'class_Acc_3': 0.2755102040816327, 'class_Acc_4': 0.6829521829521829, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6299435028248588, 'class_Acc_8': 0.6907514450867053, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6929.248, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2173   -10218.445312   0.921578    -6929.248047  ...  0.628257  0.822144  4.885041\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  73%|        | 365/500 [2:11:03<46:17, 20.58s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.882077721628374, 'Train_Acc': 0.926252278177458, 'val_Acc': 0.8256772673733804, 'test_Acc': 0.6209084836339346, 'class_Acc_0': nan, 'class_Acc_1': 0.22680412371134018, 'class_Acc_2': 0.7229166666666667, 'class_Acc_3': 0.2346938775510204, 'class_Acc_4': 0.6268191268191268, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6271186440677967, 'class_Acc_8': 0.6907514450867053, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6771.969, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2193   -10066.998047   0.926252    -6771.969238  ...  0.620908  0.825677  4.882078\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  74%|        | 368/500 [2:12:13<46:14, 21.02s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.86556092977028, 'Train_Acc': 0.9242585954093867, 'val_Acc': 0.8225363172359639, 'test_Acc': 0.6192384769539078, 'class_Acc_0': nan, 'class_Acc_1': 0.20103092783505153, 'class_Acc_2': 0.7229166666666667, 'class_Acc_3': 0.20408163265306123, 'class_Acc_4': 0.6320166320166319, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.0, 'class_Acc_7': 0.632768361581921, 'class_Acc_8': 0.6820809248554914, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7405.3916, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2213   -10615.503906   0.924259    -7405.391602  ...  0.619238  0.822536  4.865561\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  74%|        | 372/500 [2:13:38<42:09, 19.76s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.85851534632271, 'Train_Acc': 0.9212982117163412, 'val_Acc': 0.8209658421672555, 'test_Acc': 0.6219104876419506, 'class_Acc_0': nan, 'class_Acc_1': 0.20103092783505153, 'class_Acc_2': 0.6927083333333334, 'class_Acc_3': 0.21428571428571427, 'class_Acc_4': 0.6642411642411642, 'class_Acc_5': 0.761904761904762, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.632768361581921, 'class_Acc_8': 0.6965317919075144, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6977.315, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2233   -10281.892578   0.921298    -6977.314941  ...   0.62191  0.820966  4.858515\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  75%|        | 375/500 [2:14:46<42:51, 20.57s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.859993967851433, 'Train_Acc': 0.9284578280232957, 'val_Acc': 0.8268551236749117, 'test_Acc': 0.6169004676018705, 'class_Acc_0': nan, 'class_Acc_1': 0.2577319587628866, 'class_Acc_2': 0.6843750000000001, 'class_Acc_3': 0.2653061224489796, 'class_Acc_4': 0.6444906444906444, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6214689265536724, 'class_Acc_8': 0.6936416184971098, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6833.0225, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2253   -10215.041016   0.928458    -6833.022461  ...    0.6169  0.826855  4.859994\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  76%|       | 378/500 [2:15:55<42:37, 20.97s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.844445077842061, 'Train_Acc': 0.9243900787941075, 'val_Acc': 0.822928936003141, 'test_Acc': 0.6172344689378757, 'class_Acc_0': nan, 'class_Acc_1': 0.21649484536082472, 'class_Acc_2': 0.6989583333333333, 'class_Acc_3': 0.21428571428571427, 'class_Acc_4': 0.6434511434511434, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6299435028248588, 'class_Acc_8': 0.6878612716763005, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7450.9297, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2273   -10682.125977    0.92439    -7450.929688  ...  0.617234  0.822929  4.844445\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  76%|       | 382/500 [2:17:20<38:45, 19.71s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.8499747043911965, 'Train_Acc': 0.9251429941760877, 'val_Acc': 0.823321554770318, 'test_Acc': 0.6158984635938544, 'class_Acc_0': nan, 'class_Acc_1': 0.22164948453608246, 'class_Acc_2': 0.6833333333333333, 'class_Acc_3': 0.24489795918367346, 'class_Acc_4': 0.6517671517671517, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6299435028248588, 'class_Acc_8': 0.684971098265896, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7005.627, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2293   -10315.878906   0.925143    -7005.626953  ...  0.615898  0.823322  4.849975\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  77%|       | 385/500 [2:18:30<39:37, 20.68s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.840781750151158, 'Train_Acc': 0.9275893114080164, 'val_Acc': 0.8248920298390263, 'test_Acc': 0.614562458249833, 'class_Acc_0': nan, 'class_Acc_1': 0.2422680412371134, 'class_Acc_2': 0.70625, 'class_Acc_3': 0.25510204081632654, 'class_Acc_4': 0.6268191268191268, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6186440677966102, 'class_Acc_8': 0.676300578034682, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6865.7197, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2313   -10220.001953   0.927589    -6865.719727  ...  0.614562  0.824892  4.840782\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  78%|       | 388/500 [2:19:38<38:56, 20.87s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.837421566877212, 'Train_Acc': 0.9272296951010619, 'val_Acc': 0.8260698861405575, 'test_Acc': 0.6088844355377422, 'class_Acc_0': nan, 'class_Acc_1': 0.2422680412371134, 'class_Acc_2': 0.6760416666666668, 'class_Acc_3': 0.24489795918367346, 'class_Acc_4': 0.6309771309771309, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6129943502824858, 'class_Acc_8': 0.7080924855491331, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7444.3403, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc  val_Acc   val_bpd\n","2333   -10707.619141    0.92723    -7444.340332  ...  0.608884  0.82607  4.837422\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  78%|       | 392/500 [2:21:04<35:37, 19.79s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.84712312747444, 'Train_Acc': 0.9227081603288797, 'val_Acc': 0.8197879858657244, 'test_Acc': 0.624248496993988, 'class_Acc_0': nan, 'class_Acc_1': 0.18556701030927833, 'class_Acc_2': 0.6885416666666665, 'class_Acc_3': 0.22448979591836735, 'class_Acc_4': 0.6923076923076924, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6129943502824858, 'class_Acc_8': 0.6734104046242775, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7010.1104, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2353   -10316.756836   0.922708    -7010.110352  ...  0.624248  0.819788  4.847123\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  79%|      | 395/500 [2:22:13<36:04, 20.61s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.834975043499767, 'Train_Acc': 0.9299200274066461, 'val_Acc': 0.8268551236749117, 'test_Acc': 0.614562458249833, 'class_Acc_0': nan, 'class_Acc_1': 0.22164948453608246, 'class_Acc_2': 0.7083333333333334, 'class_Acc_3': 0.25510204081632654, 'class_Acc_4': 0.6351351351351351, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6101694915254238, 'class_Acc_8': 0.6676300578034682, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6876.871, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2373   -10280.838867    0.92992    -6876.871094  ...  0.614562  0.826855  4.834975\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  80%|      | 398/500 [2:23:22<35:49, 21.08s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.818167787161598, 'Train_Acc': 0.929554093867763, 'val_Acc': 0.8264625049077345, 'test_Acc': 0.6078824315297261, 'class_Acc_0': nan, 'class_Acc_1': 0.2422680412371134, 'class_Acc_2': 0.69375, 'class_Acc_3': 0.2346938775510204, 'class_Acc_4': 0.6257796257796256, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6073446327683616, 'class_Acc_8': 0.6734104046242775, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7498.285, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2393   -10749.466797   0.929554    -7498.285156  ...  0.607882  0.826463  4.818168\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  80%|      | 402/500 [2:24:47<32:23, 19.83s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.810802622341852, 'Train_Acc': 0.926634093867763, 'val_Acc': 0.8244994110718492, 'test_Acc': 0.6092184368737475, 'class_Acc_0': nan, 'class_Acc_1': 0.23195876288659792, 'class_Acc_2': 0.6958333333333334, 'class_Acc_3': 0.22448979591836735, 'class_Acc_4': 0.6216216216216216, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6186440677966102, 'class_Acc_8': 0.6878612716763005, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7079.1533, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2413   -10409.970703   0.926634     -7079.15332  ...  0.609218  0.824499  4.810803\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  81%|      | 405/500 [2:25:56<32:40, 20.64s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.8145800694647, 'Train_Acc': 0.9278518944844125, 'val_Acc': 0.8252846486062034, 'test_Acc': 0.6112224448897795, 'class_Acc_0': nan, 'class_Acc_1': 0.22164948453608246, 'class_Acc_2': 0.7010416666666666, 'class_Acc_3': 0.24489795918367346, 'class_Acc_4': 0.6247401247401246, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6129943502824858, 'class_Acc_8': 0.6878612716763005, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6921.3223, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc  val_bpd\n","2433   -10328.121094   0.927852    -6921.322266  ...  0.611222  0.825285  4.81458\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  82%|      | 408/500 [2:27:04<32:05, 20.93s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.804770259776676, 'Train_Acc': 0.9329866255566975, 'val_Acc': 0.8268551236749117, 'test_Acc': 0.6065464261857048, 'class_Acc_0': nan, 'class_Acc_1': 0.2628865979381443, 'class_Acc_2': 0.69375, 'class_Acc_3': 0.24489795918367346, 'class_Acc_4': 0.6205821205821206, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.052631578947368425, 'class_Acc_7': 0.6101694915254238, 'class_Acc_8': 0.6560693641618498, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7518.706, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc  val_bpd\n","2453   -10781.226562   0.932987    -7518.706055  ...  0.606546  0.826855  4.80477\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  82%|     | 412/500 [2:28:30<29:10, 19.89s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.792033732805183, 'Train_Acc': 0.9240585954093867, 'val_Acc': 0.8221436984687868, 'test_Acc': 0.6078824315297261, 'class_Acc_0': nan, 'class_Acc_1': 0.19072164948453607, 'class_Acc_2': 0.6895833333333334, 'class_Acc_3': 0.21428571428571427, 'class_Acc_4': 0.6382536382536382, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6242937853107345, 'class_Acc_8': 0.6676300578034682, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7119.5615, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2473   -10497.685547   0.924059    -7119.561523  ...  0.607882  0.822144  4.792034\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  83%|     | 415/500 [2:29:39<29:21, 20.73s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.80366193936766, 'Train_Acc': 0.9258063446385748, 'val_Acc': 0.8241067923046722, 'test_Acc': 0.6118904475617902, 'class_Acc_0': nan, 'class_Acc_1': 0.20103092783505153, 'class_Acc_2': 0.6958333333333334, 'class_Acc_3': 0.21428571428571427, 'class_Acc_4': 0.6455301455301455, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6214689265536724, 'class_Acc_8': 0.661849710982659, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6942.4956, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2493   -10379.330078   0.925806    -6942.495605  ...   0.61189  0.824107  4.803662\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  84%|     | 418/500 [2:30:48<28:44, 21.03s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.794575590522894, 'Train_Acc': 0.9306685440219253, 'val_Acc': 0.8252846486062034, 'test_Acc': 0.6105544422177689, 'class_Acc_0': nan, 'class_Acc_1': 0.2422680412371134, 'class_Acc_2': 0.6864583333333335, 'class_Acc_3': 0.2653061224489796, 'class_Acc_4': 0.6392931392931392, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6101694915254238, 'class_Acc_8': 0.6676300578034682, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7552.601, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2513    -10882.97168   0.930669    -7552.601074  ...  0.610554  0.825285  4.794576\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  84%|     | 422/500 [2:32:14<25:41, 19.76s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.790367495173322, 'Train_Acc': 0.9260693114080164, 'val_Acc': 0.822928936003141, 'test_Acc': 0.6115564462257849, 'class_Acc_0': nan, 'class_Acc_1': 0.19072164948453607, 'class_Acc_2': 0.7000000000000001, 'class_Acc_3': 0.22448979591836735, 'class_Acc_4': 0.6392931392931392, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6129943502824858, 'class_Acc_8': 0.676300578034682, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6671.3916, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2534   -10233.325195   0.926069    -6671.391602  ...  0.611556  0.822929  4.790367\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  85%|    | 425/500 [2:33:24<26:02, 20.83s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.788066556256384, 'Train_Acc': 0.9270170606372046, 'val_Acc': 0.8241067923046722, 'test_Acc': 0.6062124248496994, 'class_Acc_0': nan, 'class_Acc_1': 0.19072164948453607, 'class_Acc_2': 0.7020833333333335, 'class_Acc_3': 0.21428571428571427, 'class_Acc_4': 0.6278586278586278, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6101694915254238, 'class_Acc_8': 0.661849710982659, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6999.3784, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2554   -10443.280273   0.927017    -6999.378418  ...  0.606212  0.824107  4.788067\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  86%|    | 429/500 [2:34:49<23:16, 19.67s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.7782785842374365, 'Train_Acc': 0.9255100787941075, 'val_Acc': 0.822928936003141, 'test_Acc': 0.6062124248496994, 'class_Acc_0': nan, 'class_Acc_1': 0.19072164948453607, 'class_Acc_2': 0.6791666666666667, 'class_Acc_3': 0.21428571428571427, 'class_Acc_4': 0.6476091476091476, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6073446327683616, 'class_Acc_8': 0.6734104046242775, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7147.148, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2575   -10588.011719    0.92551    -7147.147949  ...  0.606212  0.822929  4.778279\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  86%|    | 432/500 [2:35:58<23:10, 20.45s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.784390783442244, 'Train_Acc': 0.9266962932511134, 'val_Acc': 0.822928936003141, 'test_Acc': 0.6085504342017368, 'class_Acc_0': nan, 'class_Acc_1': 0.1804123711340206, 'class_Acc_2': 0.6958333333333334, 'class_Acc_3': 0.22448979591836735, 'class_Acc_4': 0.6434511434511434, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6016949152542374, 'class_Acc_8': 0.6676300578034682, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7004.631, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2596   -10475.605469   0.926696    -7004.630859  ...   0.60855  0.822929  4.784391\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  87%|    | 436/500 [2:37:23<20:52, 19.57s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.776092938820673, 'Train_Acc': 0.9264704624871531, 'val_Acc': 0.8248920298390263, 'test_Acc': 0.603874415497662, 'class_Acc_0': nan, 'class_Acc_1': 0.19072164948453607, 'class_Acc_2': 0.69375, 'class_Acc_3': 0.20408163265306123, 'class_Acc_4': 0.6309771309771309, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6016949152542374, 'class_Acc_8': 0.6676300578034682, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7151.9023, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2617   -10550.041992    0.92647    -7151.902344  ...  0.603874  0.824892  4.776093\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  88%|    | 439/500 [2:38:33<20:47, 20.44s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.778027098822593, 'Train_Acc': 0.9294800274066461, 'val_Acc': 0.8252846486062034, 'test_Acc': 0.6052104208416834, 'class_Acc_0': nan, 'class_Acc_1': 0.20618556701030927, 'class_Acc_2': 0.6822916666666666, 'class_Acc_3': 0.22448979591836735, 'class_Acc_4': 0.6330561330561331, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6129943502824858, 'class_Acc_8': 0.6791907514450868, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7019.064, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2638   -10515.823242    0.92948    -7019.063965  ...   0.60521  0.825285  4.778027\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  89%|   | 443/500 [2:39:58<18:37, 19.61s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.774419421965788, 'Train_Acc': 0.9285370606372045, 'val_Acc': 0.8244994110718492, 'test_Acc': 0.603874415497662, 'class_Acc_0': nan, 'class_Acc_1': 0.20618556701030927, 'class_Acc_2': 0.6854166666666666, 'class_Acc_3': 0.21428571428571427, 'class_Acc_4': 0.6382536382536382, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.5932203389830509, 'class_Acc_8': 0.6676300578034682, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7155.547, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2659   -10587.393555   0.928537    -7155.546875  ...  0.603874  0.824499  4.774419\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  89%|   | 446/500 [2:41:08<18:25, 20.47s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.769663976007441, 'Train_Acc': 0.9269362932511134, 'val_Acc': 0.822928936003141, 'test_Acc': 0.6042084168336673, 'class_Acc_0': nan, 'class_Acc_1': 0.19587628865979378, 'class_Acc_2': 0.6791666666666667, 'class_Acc_3': 0.22448979591836735, 'class_Acc_4': 0.6444906444906444, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.5960451977401131, 'class_Acc_8': 0.6705202312138728, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7014.8164, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2679   -10495.749023   0.926936    -7014.816406  ...  0.604208  0.822929  4.769664\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  90%|   | 449/500 [2:42:16<17:31, 20.62s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.771875685738907, 'Train_Acc': 0.9297085440219254, 'val_Acc': 0.8260698861405575, 'test_Acc': 0.6035404141616566, 'class_Acc_0': nan, 'class_Acc_1': 0.211340206185567, 'class_Acc_2': 0.6927083333333334, 'class_Acc_3': 0.21428571428571427, 'class_Acc_4': 0.6257796257796256, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.5988700564971752, 'class_Acc_8': 0.6705202312138728, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7597.571, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc  val_Acc   val_bpd\n","2699   -10947.337891   0.929709    -7597.570801  ...   0.60354  0.82607  4.771876\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  91%|   | 453/500 [2:43:43<15:27, 19.74s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.762947366477885, 'Train_Acc': 0.9287418430969511, 'val_Acc': 0.8241067923046722, 'test_Acc': 0.6018704074816299, 'class_Acc_0': nan, 'class_Acc_1': 0.20618556701030927, 'class_Acc_2': 0.6885416666666665, 'class_Acc_3': 0.21428571428571427, 'class_Acc_4': 0.6340956340956341, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.5847457627118644, 'class_Acc_8': 0.661849710982659, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6730.292, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2720   -10300.919922   0.928742    -6730.291992  ...   0.60187  0.824107  4.762947\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  91%|  | 456/500 [2:44:52<15:01, 20.49s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.762467407385506, 'Train_Acc': 0.9295877766358341, 'val_Acc': 0.8260698861405575, 'test_Acc': 0.6032064128256514, 'class_Acc_0': nan, 'class_Acc_1': 0.20103092783505153, 'class_Acc_2': 0.6864583333333335, 'class_Acc_3': 0.2346938775510204, 'class_Acc_4': 0.6340956340956341, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.5932203389830509, 'class_Acc_8': 0.6676300578034682, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7048.864, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc  val_Acc   val_bpd\n","2740   -10561.082031   0.929588     -7048.86377  ...  0.603206  0.82607  4.762467\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  92%|  | 460/500 [2:46:18<13:06, 19.66s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.764256922181504, 'Train_Acc': 0.9290396437136005, 'val_Acc': 0.8252846486062034, 'test_Acc': 0.6052104208416834, 'class_Acc_0': nan, 'class_Acc_1': 0.211340206185567, 'class_Acc_2': 0.6843750000000001, 'class_Acc_3': 0.22448979591836735, 'class_Acc_4': 0.6382536382536382, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.5988700564971752, 'class_Acc_8': 0.6705202312138728, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7176.944, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2761    -10635.31543    0.92904    -7176.943848  ...   0.60521  0.825285  4.764257\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  93%|  | 463/500 [2:47:28<12:44, 20.67s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.760177035082636, 'Train_Acc': 0.9281429941760877, 'val_Acc': 0.8252846486062034, 'test_Acc': 0.6032064128256514, 'class_Acc_0': nan, 'class_Acc_1': 0.20618556701030927, 'class_Acc_2': 0.6854166666666666, 'class_Acc_3': 0.22448979591836735, 'class_Acc_4': 0.6330561330561331, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.6016949152542374, 'class_Acc_8': 0.6647398843930636, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7033.978, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2781   -10515.039062   0.928143    -7033.978027  ...  0.603206  0.825285  4.760177\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  93%|  | 467/500 [2:48:55<10:54, 19.84s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.757787571421163, 'Train_Acc': 0.928896676944159, 'val_Acc': 0.8260698861405575, 'test_Acc': 0.6002004008016032, 'class_Acc_0': nan, 'class_Acc_1': 0.20618556701030927, 'class_Acc_2': 0.6854166666666666, 'class_Acc_3': 0.22448979591836735, 'class_Acc_4': 0.6320166320166319, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.5875706214689266, 'class_Acc_8': 0.6560693641618498, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7287.221, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc  val_Acc   val_bpd\n","2802   -10723.647461   0.928897    -7287.221191  ...    0.6002  0.82607  4.757788\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  94%|  | 470/500 [2:50:05<10:19, 20.66s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.7559712878695235, 'Train_Acc': 0.9290229941760877, 'val_Acc': 0.8260698861405575, 'test_Acc': 0.5995323981295925, 'class_Acc_0': nan, 'class_Acc_1': 0.20618556701030927, 'class_Acc_2': 0.6843750000000001, 'class_Acc_3': 0.21428571428571427, 'class_Acc_4': 0.6278586278586278, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.5875706214689266, 'class_Acc_8': 0.6676300578034682, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6743.8535, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc  val_Acc   val_bpd\n","2822   -10343.595703   0.929023    -6743.853516  ...  0.599532  0.82607  4.755971\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  95%| | 473/500 [2:51:13<09:24, 20.89s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.7576452743517015, 'Train_Acc': 0.9294281603288798, 'val_Acc': 0.8244994110718492, 'test_Acc': 0.6002004008016032, 'class_Acc_0': nan, 'class_Acc_1': 0.20618556701030927, 'class_Acc_2': 0.6833333333333333, 'class_Acc_3': 0.21428571428571427, 'class_Acc_4': 0.6299376299376299, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.5903954802259886, 'class_Acc_8': 0.6676300578034682, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7059.963, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2842   -10582.264648   0.929428    -7059.962891  ...    0.6002  0.824499  4.757645\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  95%| | 477/500 [2:52:41<07:42, 20.12s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.7573480002758926, 'Train_Acc': 0.9290463446385748, 'val_Acc': 0.8256772673733804, 'test_Acc': 0.6012024048096193, 'class_Acc_0': nan, 'class_Acc_1': 0.20618556701030927, 'class_Acc_2': 0.6843750000000001, 'class_Acc_3': 0.21428571428571427, 'class_Acc_4': 0.6288981288981288, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.5932203389830509, 'class_Acc_8': 0.6734104046242775, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7289.379, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2862   -10697.394531   0.929046    -7289.378906  ...  0.601202  0.825677  4.757348\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  96%| | 480/500 [2:53:50<06:54, 20.72s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.755982089297238, 'Train_Acc': 0.9287267283316204, 'val_Acc': 0.8252846486062034, 'test_Acc': 0.6012024048096193, 'class_Acc_0': nan, 'class_Acc_1': 0.21649484536082472, 'class_Acc_2': 0.6864583333333335, 'class_Acc_3': 0.21428571428571427, 'class_Acc_4': 0.6278586278586278, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.5903954802259886, 'class_Acc_8': 0.6676300578034682, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6742.998, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2882   -10329.886719   0.928727    -6742.998047  ...  0.601202  0.825285  4.755982\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  97%| | 483/500 [2:54:59<05:55, 20.91s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.755894973434581, 'Train_Acc': 0.9289207947927373, 'val_Acc': 0.8252846486062034, 'test_Acc': 0.6002004008016032, 'class_Acc_0': nan, 'class_Acc_1': 0.211340206185567, 'class_Acc_2': 0.6864583333333335, 'class_Acc_3': 0.21428571428571427, 'class_Acc_4': 0.6268191268191268, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.5875706214689266, 'class_Acc_8': 0.6676300578034682, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7628.0923, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2903   -10956.763672   0.928921    -7628.092285  ...    0.6002  0.825285  4.755895\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  97%|| 487/500 [2:56:24<04:16, 19.72s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.755476535517448, 'Train_Acc': 0.9295937101747174, 'val_Acc': 0.8252846486062034, 'test_Acc': 0.5991983967935872, 'class_Acc_0': nan, 'class_Acc_1': 0.211340206185567, 'class_Acc_2': 0.6864583333333335, 'class_Acc_3': 0.21428571428571427, 'class_Acc_4': 0.6247401247401246, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.5875706214689266, 'class_Acc_8': 0.6647398843930636, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-6744.653, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2924   -10328.882812   0.929594    -6744.652832  ...  0.599198  0.825285  4.755477\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  98%|| 490/500 [2:57:33<03:25, 20.56s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.753914555144402, 'Train_Acc': 0.9290515107913669, 'val_Acc': 0.8248920298390263, 'test_Acc': 0.5991983967935872, 'class_Acc_0': nan, 'class_Acc_1': 0.20618556701030927, 'class_Acc_2': 0.6875, 'class_Acc_3': 0.21428571428571427, 'class_Acc_4': 0.6257796257796256, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.5847457627118644, 'class_Acc_8': 0.6647398843930636, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7066.7236, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2944   -10564.452148   0.929052    -7066.723633  ...  0.599198  0.824892  4.753915\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  99%|| 494/500 [2:58:59<01:58, 19.69s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.754952431459625, 'Train_Acc': 0.9289718944844125, 'val_Acc': 0.8248920298390263, 'test_Acc': 0.5988643954575819, 'class_Acc_0': nan, 'class_Acc_1': 0.211340206185567, 'class_Acc_2': 0.6864583333333335, 'class_Acc_3': 0.20408163265306123, 'class_Acc_4': 0.6247401247401246, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.5875706214689266, 'class_Acc_8': 0.6647398843930636, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7195.144, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2965   -10586.860352   0.928972    -7195.144043  ...  0.598864  0.824892  4.754952\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train:  99%|| 497/500 [3:00:07<01:01, 20.51s/it]/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.754577434066998, 'Train_Acc': 0.9290289277149709, 'val_Acc': 0.8248920298390263, 'test_Acc': 0.5991983967935872, 'class_Acc_0': nan, 'class_Acc_1': 0.211340206185567, 'class_Acc_2': 0.6864583333333335, 'class_Acc_3': 0.20408163265306123, 'class_Acc_4': 0.6268191268191268, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.5875706214689266, 'class_Acc_8': 0.661849710982659, 'class_Acc_9': nan, 'Unlab_loss(mb)': array(-7065.5156, dtype=float32)}\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2986   -10480.679688   0.929029    -7065.515625  ...  0.599198  0.824892  4.754577\n","\n","[1 rows x 17 columns]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","train: 100%|| 500/500 [3:01:16<00:00, 21.75s/it]\n","/home/anunay18021/SingleCellClassification/flowgmm/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n","/home/anunay18021/SingleCellClassification/flowgmm/experiments/train_flows/train_semisup_flowgmm_tabular.py:90: RuntimeWarning: invalid value encountered in double_scalars\n","  class_wise_acc.append(lambda mb:  (torch.mul(self.model.prior.classify(self.model(mb[0]))\n","{'val_bpd': 4.755121497284729, 'Train_Acc': 0.9289548612538541, 'val_Acc': 0.8256772673733804, 'test_Acc': 0.5991983967935872, 'class_Acc_0': nan, 'class_Acc_1': 0.211340206185567, 'class_Acc_2': 0.6843750000000001, 'class_Acc_3': 0.20408163265306123, 'class_Acc_4': 0.6268191268191268, 'class_Acc_5': 0.7857142857142858, 'class_Acc_6': 0.026315789473684213, 'class_Acc_7': 0.5875706214689266, 'class_Acc_8': 0.6676300578034682, 'class_Acc_9': nan}\n","/home/anunay18021/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","      Minibatch_Loss  Train_Acc  Unlab_loss(mb)  ...  test_Acc   val_Acc   val_bpd\n","2999             NaN   0.928955             NaN  ...  0.599198  0.825677  4.755121\n","\n","[1 rows x 17 columns]\n"]}],"source":["!python3 experiments/train_flows/flowgmm_tabular_new.py --num_classes 10 --metric_name \"10x Chromium (v3)\" --trainer_config \"{'unlab_weight':.6}\" --net_config \"{'k':1024,'coupling_layers':7,'nperlayer':1}\" --network RealNVPTabularWPrior --trainer SemiFlow --num_epochs 500 --dataset AG_News --lr 3e-4 --train 200"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["with open(\"/home/anunay18021/SingleCellClassification/tmp/metrics_\" + batch_name + \".pkl\", \"rb\") as f:\n","  D = pickle.load(f)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["rev = list(mapping.keys())\n","new_D = {}\n","for i in D:\n","  try:\n","    new_D[reverse_mapping[int(i.split(\"_\")[-1])]] = D[i]\n","  except:\n","    continue"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["plt.rcParams[\"figure.figsize\"] = (20,5.5)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 2000x550 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plt.bar(range(len(new_D)), list(new_D.values()))\n","plt.xticks(range(len(new_D)), list(new_D.keys()))\n","plt.show()"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["classes_labeled = {}\n","for i in D:\n","  try:\n","        classes_labeled[reverse_mapping[int(i.split(\"_\")[-1])]] = D[i]\n","  except:\n","        classes_labeled[i] = D[i]"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["df = pd.DataFrame(columns = list(classes_labeled.keys()))"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_292779/3840281218.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df.append(classes_labeled, ignore_index = True)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>val_bpd</th>\n","      <th>Train_Acc</th>\n","      <th>val_Acc</th>\n","      <th>test_Acc</th>\n","      <th>Plasmacytoid dendritic cell</th>\n","      <th>Natural killer cell</th>\n","      <th>CD4+ T cell</th>\n","      <th>CD16+ monocyte</th>\n","      <th>Cytotoxic T cell</th>\n","      <th>Megakaryocyte</th>\n","      <th>Dendritic cell</th>\n","      <th>CD14+ monocyte</th>\n","      <th>B cell</th>\n","      <th>Unassigned</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4.755121</td>\n","      <td>0.928955</td>\n","      <td>0.825677</td>\n","      <td>0.599198</td>\n","      <td>NaN</td>\n","      <td>0.21134</td>\n","      <td>0.684375</td>\n","      <td>0.204082</td>\n","      <td>0.626819</td>\n","      <td>0.785714</td>\n","      <td>0.026316</td>\n","      <td>0.587571</td>\n","      <td>0.66763</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    val_bpd  Train_Acc   val_Acc  test_Acc  Plasmacytoid dendritic cell  \\\n","0  4.755121   0.928955  0.825677  0.599198                          NaN   \n","\n","   Natural killer cell  CD4+ T cell  CD16+ monocyte  Cytotoxic T cell  \\\n","0              0.21134     0.684375        0.204082          0.626819   \n","\n","   Megakaryocyte  Dendritic cell  CD14+ monocyte   B cell  Unassigned  \n","0       0.785714        0.026316        0.587571  0.66763         NaN  "]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["df.append(classes_labeled, ignore_index = True)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1fTsVaFW9rVur8hpaEbz5dsxiGXSEy9Sk","timestamp":1679851239019}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
